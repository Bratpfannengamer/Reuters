{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=pd.read_csv('ModApte_train.csv')\n",
    "df_test=pd.read_csv('ModApte_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_to_list(df,column_name):\n",
    "    result=df[column_name].replace({' ':''},regex=True)\n",
    "    result.replace({'\\\\n':''},regex=True,inplace=True)\n",
    "    result.replace({'\\'\\'':'\\',\\''},regex=True,inplace=True)\n",
    "    return result.apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9603 entries, 0 to 9602\n",
      "Data columns (total 13 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   text         8816 non-null   object\n",
      " 1   text_type    9603 non-null   object\n",
      " 2   topics       9603 non-null   object\n",
      " 3   lewis_split  9603 non-null   object\n",
      " 4   cgis_split   9603 non-null   object\n",
      " 5   old_id       9603 non-null   object\n",
      " 6   new_id       9603 non-null   object\n",
      " 7   places       9603 non-null   object\n",
      " 8   people       9603 non-null   object\n",
      " 9   orgs         9603 non-null   object\n",
      " 10  exchanges    9603 non-null   object\n",
      " 11  date         9603 non-null   object\n",
      " 12  title        9549 non-null   object\n",
      "dtypes: object(13)\n",
      "memory usage: 975.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text           object\n",
       "text_type      object\n",
       "topics         object\n",
       "lewis_split    object\n",
       "cgis_split     object\n",
       "old_id         object\n",
       "new_id         object\n",
       "places         object\n",
       "people         object\n",
       "orgs           object\n",
       "exchanges      object\n",
       "date           object\n",
       "title          object\n",
       "dtype: object"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_type</th>\n",
       "      <th>topics</th>\n",
       "      <th>lewis_split</th>\n",
       "      <th>cgis_split</th>\n",
       "      <th>old_id</th>\n",
       "      <th>new_id</th>\n",
       "      <th>places</th>\n",
       "      <th>people</th>\n",
       "      <th>orgs</th>\n",
       "      <th>exchanges</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Showers continued throughout the week in\\nthe ...</td>\n",
       "      <td>\"NORM\"</td>\n",
       "      <td>['cocoa']</td>\n",
       "      <td>\"TRAIN\"</td>\n",
       "      <td>\"TRAINING-SET\"</td>\n",
       "      <td>\"5544\"</td>\n",
       "      <td>\"1\"</td>\n",
       "      <td>['el-salvador' 'usa' 'uruguay']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>26-FEB-1987 15:01:01.79</td>\n",
       "      <td>BAHIA COCOA REVIEW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The U.S. Agriculture Department\\nreported the ...</td>\n",
       "      <td>\"NORM\"</td>\n",
       "      <td>['grain' 'wheat' 'corn' 'barley' 'oat' 'sorghum']</td>\n",
       "      <td>\"TRAIN\"</td>\n",
       "      <td>\"TRAINING-SET\"</td>\n",
       "      <td>\"5548\"</td>\n",
       "      <td>\"5\"</td>\n",
       "      <td>['usa']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>26-FEB-1987 15:10:44.60</td>\n",
       "      <td>NATIONAL AVERAGE PRICES FOR FARMER-OWNED RESERVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Argentine grain board figures show\\ncrop regis...</td>\n",
       "      <td>\"NORM\"</td>\n",
       "      <td>['veg-oil' 'linseed' 'lin-oil' 'soy-oil' 'sun-...</td>\n",
       "      <td>\"TRAIN\"</td>\n",
       "      <td>\"TRAINING-SET\"</td>\n",
       "      <td>\"5549\"</td>\n",
       "      <td>\"6\"</td>\n",
       "      <td>['argentina']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>26-FEB-1987 15:14:36.41</td>\n",
       "      <td>ARGENTINE 1986/87 GRAIN/OILSEED REGISTRATIONS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Moody's Investors Service Inc said it\\nlowered...</td>\n",
       "      <td>\"NORM\"</td>\n",
       "      <td>[]</td>\n",
       "      <td>\"TRAIN\"</td>\n",
       "      <td>\"TRAINING-SET\"</td>\n",
       "      <td>\"5551\"</td>\n",
       "      <td>\"8\"</td>\n",
       "      <td>['usa']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>26-FEB-1987 15:15:40.12</td>\n",
       "      <td>USX &amp;lt;X&gt; DEBT DOWGRADED BY MOODY'S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Champion Products Inc said its\\nboard of direc...</td>\n",
       "      <td>\"NORM\"</td>\n",
       "      <td>['earn']</td>\n",
       "      <td>\"TRAIN\"</td>\n",
       "      <td>\"TRAINING-SET\"</td>\n",
       "      <td>\"5552\"</td>\n",
       "      <td>\"9\"</td>\n",
       "      <td>['usa']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>26-FEB-1987 15:17:11.20</td>\n",
       "      <td>CHAMPION PRODUCTS &amp;lt;CH&gt; APPROVES STOCK SPLIT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text text_type  \\\n",
       "0  Showers continued throughout the week in\\nthe ...    \"NORM\"   \n",
       "1  The U.S. Agriculture Department\\nreported the ...    \"NORM\"   \n",
       "2  Argentine grain board figures show\\ncrop regis...    \"NORM\"   \n",
       "3  Moody's Investors Service Inc said it\\nlowered...    \"NORM\"   \n",
       "4  Champion Products Inc said its\\nboard of direc...    \"NORM\"   \n",
       "\n",
       "                                              topics lewis_split  \\\n",
       "0                                          ['cocoa']     \"TRAIN\"   \n",
       "1  ['grain' 'wheat' 'corn' 'barley' 'oat' 'sorghum']     \"TRAIN\"   \n",
       "2  ['veg-oil' 'linseed' 'lin-oil' 'soy-oil' 'sun-...     \"TRAIN\"   \n",
       "3                                                 []     \"TRAIN\"   \n",
       "4                                           ['earn']     \"TRAIN\"   \n",
       "\n",
       "       cgis_split  old_id new_id                           places people orgs  \\\n",
       "0  \"TRAINING-SET\"  \"5544\"    \"1\"  ['el-salvador' 'usa' 'uruguay']     []   []   \n",
       "1  \"TRAINING-SET\"  \"5548\"    \"5\"                          ['usa']     []   []   \n",
       "2  \"TRAINING-SET\"  \"5549\"    \"6\"                    ['argentina']     []   []   \n",
       "3  \"TRAINING-SET\"  \"5551\"    \"8\"                          ['usa']     []   []   \n",
       "4  \"TRAINING-SET\"  \"5552\"    \"9\"                          ['usa']     []   []   \n",
       "\n",
       "  exchanges                     date  \\\n",
       "0        []  26-FEB-1987 15:01:01.79   \n",
       "1        []  26-FEB-1987 15:10:44.60   \n",
       "2        []  26-FEB-1987 15:14:36.41   \n",
       "3        []  26-FEB-1987 15:15:40.12   \n",
       "4        []  26-FEB-1987 15:17:11.20   \n",
       "\n",
       "                                              title  \n",
       "0                                BAHIA COCOA REVIEW  \n",
       "1  NATIONAL AVERAGE PRICES FOR FARMER-OWNED RESERVE  \n",
       "2     ARGENTINE 1986/87 GRAIN/OILSEED REGISTRATIONS  \n",
       "3              USX &lt;X> DEBT DOWGRADED BY MOODY'S  \n",
       "4    CHAMPION PRODUCTS &lt;CH> APPROVES STOCK SPLIT  "
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define topics\n",
    "topic_column = 'topics'\n",
    "food = ['coconut', 'cotton-oil', 'sorghum', 'orange', 'rice', 'soybean', 'sun-meal', \n",
    "    'oilseed', 'sugar', 'hog', 'coffee', 'groundnut', 'sunseed', 'sun-oil', 'rye', \n",
    "    'lin-oil', 'copra-cake', 'potato', 'barley', 'tea', 'meal-feed', 'coconut-oil', \n",
    "    'palmkernel', 'cottonseed', 'castor-oil', 'l-cattle', 'livestock', 'soy-oil', \n",
    "    'rape-oil', 'palm-oil', 'cocoa', 'cotton', 'wheat', 'corn', 'f-cattle', 'grain', \n",
    "    'soy-meal', 'oat', 'groundnut-oil', 'veg-oil','rapeseed']\n",
    "resource = ['platinum', 'lead', 'nickel', 'strategic-metal', 'copper', 'palladium', 'gold', \n",
    "    'zinc', 'tin', 'iron-steel', 'alum', 'silver', 'nat-gas', 'rubber', 'pet-chem', 'fuel', 'crude','lumber','propane']\n",
    "finance = ['money-supply', 'dlr', 'nkr', 'lei', 'yen', 'dfl', 'sfr', 'cpi', 'instal-debt', \n",
    "    'money-fx', 'gnp', 'interest', 'income', 'dmk', 'rand', 'bop', 'reserves', 'nzdlr','acq']\n",
    "personal_finance = ['housing','jobs','earn']\n",
    "transport = ['jet', 'ship']\n",
    "topics=[[food,'food'],[resource,'resource'],[finance,'finance'],[personal_finance,'personal_finance'],[transport,'transport']]\n",
    "topics_to_remove = ['gas', 'heat', 'trade', 'retail', 'carcass', 'cpu', 'wpi', 'naphtha', 'ipi']\n",
    "\n",
    "#columns with special treatment\n",
    "list_columns=['places']\n",
    "drop_columns=['text_type','people','orgs','exchanges','lewis_split','cgis_split','old_id','new_id']\n",
    "notnan_columns=['text','topics']\n",
    "date_columns=['date']\n",
    "text_columns=['text','title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_row_notnan_columms(df,notnan_columns):\n",
    "    for column in notnan_columns:\n",
    "        df[column].dropna(inplace=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funtion to reorganize a column of subtopics into  broader topics and removing some of them \n",
    "def categorize_topics(df,column,topics,remove):\n",
    "    for topic in topics:\n",
    "        for subtopic in topic[0]:\n",
    "            df[column]=df[column].replace({'\\''+subtopic+'\\'':'\\''+topic[1] +'\\''},regex=True)\n",
    "    for subtopic in remove:\n",
    "        df[column]=df[column].replace({'\\''+subtopic+'\\'':''},regex=True)\n",
    "    df[column]=df[column].replace({' ':''},regex=True)\n",
    "    df[column]=series_to_list(df,column)\n",
    "    df=df[df[column].str.len()==1]\n",
    "    df[column]=df[column].apply(lambda x : x[0])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explode_listcolumns(df,list_columns):\n",
    "    for column in list_columns:\n",
    "        df[column]=series_to_list(df,column)\n",
    "        df = df.join(pd.crosstab((s:=df[column].explode()).index, s).add_prefix(column+'_'))\n",
    "        df = df.drop(columns=column)\n",
    "        for col  in [col for col in df if col.startswith(column+'_')]:\n",
    "            df[col].fillna(value=0,inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_datecolumns(df,date_columns):\n",
    "    for column in date_columns:\n",
    "        # Die Zeichenkette in ein Datum konvertieren\n",
    "        df[column] = pd.to_datetime(df[column])\n",
    "        df[column+'_month'] = df[column].dt.month\n",
    "        df[column+'_month'] = (df[column+'_month'] - df[column+'_month'].mean()) / df[column+'_month'].std()\n",
    "\n",
    "        # Woche extrahieren (altes Verhalten, ab Pandas 1.1.0 ist isocalendar().week empfohlen)\n",
    "        df[column+'_day_month'] = df[column].dt.day\n",
    "        df[column+'_day_month'] = (df[column+'_day_month'] - df[column+'_day_month'].mean()) / df[column+'_day_month'].std()\n",
    "\n",
    "        # Tag extrahieren\n",
    "        df[column+'_day_year'] = df[column].dt.day_of_year\n",
    "        df[column+'_day_year'] = (df[column+'_day_year'] - df[column+'_day_year'].mean()) / df[column+'_day_year'].std()\n",
    "\n",
    "\n",
    "        # Wochentag extrahieren (Montag=0, Sonntag=6)\n",
    "        df[column+'_weekday'] = df[column].dt.day_name('en')\n",
    "\n",
    "        df[column+'_quarter_year'] = df[column].dt.quarter\n",
    "        df=pd.get_dummies(df, columns=[column+'_weekday'])\n",
    "        weekdays=['weekday_Monday','weekday_Tuesday','weekday_Wednesday','weekday_Thursday','weekday_Friday','weekday_Saturday','weekday_Sunday']\n",
    "        for weekday in weekdays:\n",
    "            if not column+'_'+weekday in df.columns:\n",
    "                df[column+'_'+weekday]=0\n",
    "            else:\n",
    "                df[column+'_'+weekday]=df[column+'_'+weekday].astype(int)\n",
    "        \n",
    "        df=df.drop(columns=column)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_textcolumns(df,text_columns):\n",
    "    for column in text_columns:\n",
    "        df[column].replace({'&lt;':'<'},regex=True,inplace=True)\n",
    "        df[column].replace({'\\\\n':' '},regex=True,inplace=True)\n",
    "        df[column]=df['text'].str.replace('\\s+', ' ', regex=True)\n",
    "        df[column]=df[column].str.lower()\n",
    "        df[column]=df[column].fillna(value='')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_special_columns(df,list_columns,drop_columns,date_columns,notnan_columns,text_columns):\n",
    "    df=df.drop(columns=drop_columns)\n",
    "    df= explode_listcolumns(df,list_columns)\n",
    "    df= format_datecolumns(df,date_columns)\n",
    "    df= drop_row_notnan_columms(df,notnan_columns)\n",
    "    df= format_textcolumns(df,text_columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "#has to expanded to make it readeble by model\n",
    "def preprocessing(df,topic_column,topics,topics_to_remove,list_columns,drop_columns,date_columns,notnan_columns,text_columns,encoder,fit_encoder=True):\n",
    "    df=categorize_topics(df,topic_column,topics,topics_to_remove)\n",
    "    df = handle_special_columns(df,list_columns,drop_columns,date_columns,notnan_columns,text_columns)\n",
    "    \n",
    "    #create torch tensor \n",
    "    additional_features = torch.tensor(df.drop(columns=(text_columns+[topic_column])).values)\n",
    "\n",
    "    #labels\n",
    "    if fit_encoder:\n",
    "        labels = torch.tensor(encoder.fit_transform(df[topic_column]))\n",
    "    else:\n",
    "        labels = torch.tensor(encoder.transform(df[topic_column]))\n",
    "    return df[text_columns],additional_features,labels,encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\theinrich\\AppData\\Local\\Temp\\ipykernel_24060\\426969037.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column]=df[column].apply(lambda x : x[0])\n",
      "C:\\Users\\theinrich\\AppData\\Local\\Temp\\ipykernel_24060\\2598233951.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(value=0,inplace=True)\n",
      "C:\\Users\\theinrich\\AppData\\Local\\Temp\\ipykernel_24060\\1504739334.py:4: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[column] = pd.to_datetime(df[column])\n"
     ]
    },
    {
     "ename": "DateParseError",
     "evalue": "Unknown datetime string format, unable to parse: 27-MAR-1987 00:03:35.68&#5;&#5;&#5;F, at position 4858",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDateParseError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[214], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_df_text,train_additional_features,train_labels,label_encoder \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtopic_column\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtopics\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtopics_to_remove\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlist_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdrop_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdate_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnotnan_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtext_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlabel_encoder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m test_df_text,test_additional_features,test_labels,label_encoder \u001b[38;5;241m=\u001b[39m preprocessing(df_test,topic_column,topics,topics_to_remove,list_columns,drop_columns,date_columns,notnan_columns,text_columns,label_encoder,fit_encoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[212], line 4\u001b[0m, in \u001b[0;36mpreprocessing\u001b[1;34m(df, topic_column, topics, topics_to_remove, list_columns, drop_columns, date_columns, notnan_columns, text_columns, encoder, fit_encoder)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocessing\u001b[39m(df,topic_column,topics,topics_to_remove,list_columns,drop_columns,date_columns,notnan_columns,text_columns,encoder,fit_encoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m      3\u001b[0m     df\u001b[38;5;241m=\u001b[39mcategorize_topics(df,topic_column,topics,topics_to_remove)\n\u001b[1;32m----> 4\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mhandle_special_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlist_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdrop_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdate_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnotnan_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtext_columns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m#create torch tensor \u001b[39;00m\n\u001b[0;32m      7\u001b[0m     additional_features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(df\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m(text_columns\u001b[38;5;241m+\u001b[39m[topic_column]))\u001b[38;5;241m.\u001b[39mvalues)\n",
      "Cell \u001b[1;32mIn[211], line 4\u001b[0m, in \u001b[0;36mhandle_special_columns\u001b[1;34m(df, list_columns, drop_columns, date_columns, notnan_columns, text_columns)\u001b[0m\n\u001b[0;32m      2\u001b[0m df\u001b[38;5;241m=\u001b[39mdf\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39mdrop_columns)\n\u001b[0;32m      3\u001b[0m df\u001b[38;5;241m=\u001b[39m explode_listcolumns(df,list_columns)\n\u001b[1;32m----> 4\u001b[0m df\u001b[38;5;241m=\u001b[39m \u001b[43mformat_datecolumns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdate_columns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m df\u001b[38;5;241m=\u001b[39m drop_row_notnan_columms(df,notnan_columns)\n\u001b[0;32m      6\u001b[0m df\u001b[38;5;241m=\u001b[39m format_textcolumns(df,text_columns)\n",
      "Cell \u001b[1;32mIn[209], line 4\u001b[0m, in \u001b[0;36mformat_datecolumns\u001b[1;34m(df, date_columns)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat_datecolumns\u001b[39m(df,date_columns):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m column \u001b[38;5;129;01min\u001b[39;00m date_columns:\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;66;03m# Die Zeichenkette in ein Datum konvertieren\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m         df[column] \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_datetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m         df[column\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_month\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[column]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mmonth\n\u001b[0;32m      6\u001b[0m         df[column\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_month\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (df[column\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_month\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m-\u001b[39m df[column\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_month\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()) \u001b[38;5;241m/\u001b[39m df[column\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_month\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstd()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:1067\u001b[0m, in \u001b[0;36mto_datetime\u001b[1;34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[0;32m   1065\u001b[0m         result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39mmap(cache_array)\n\u001b[0;32m   1066\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1067\u001b[0m         values \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_listlike\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1068\u001b[0m         result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39m_constructor(values, index\u001b[38;5;241m=\u001b[39marg\u001b[38;5;241m.\u001b[39mindex, name\u001b[38;5;241m=\u001b[39marg\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1069\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, (ABCDataFrame, abc\u001b[38;5;241m.\u001b[39mMutableMapping)):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:435\u001b[0m, in \u001b[0;36m_convert_listlike_datetimes\u001b[1;34m(arg, format, name, utc, unit, errors, dayfirst, yearfirst, exact)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmixed\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _array_strptime_with_fallback(arg, name, utc, \u001b[38;5;28mformat\u001b[39m, exact, errors)\n\u001b[1;32m--> 435\u001b[0m result, tz_parsed \u001b[38;5;241m=\u001b[39m \u001b[43mobjects_to_datetime64\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    436\u001b[0m \u001b[43m    \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    437\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdayfirst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdayfirst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    438\u001b[0m \u001b[43m    \u001b[49m\u001b[43myearfirst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43myearfirst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    439\u001b[0m \u001b[43m    \u001b[49m\u001b[43mutc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mutc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    440\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    441\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_object\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    442\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tz_parsed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    445\u001b[0m     \u001b[38;5;66;03m# We can take a shortcut since the datetime64 numpy array\u001b[39;00m\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;66;03m# is in UTC\u001b[39;00m\n\u001b[0;32m    447\u001b[0m     out_unit \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdatetime_data(result\u001b[38;5;241m.\u001b[39mdtype)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\arrays\\datetimes.py:2398\u001b[0m, in \u001b[0;36mobjects_to_datetime64\u001b[1;34m(data, dayfirst, yearfirst, utc, errors, allow_object, out_unit)\u001b[0m\n\u001b[0;32m   2395\u001b[0m \u001b[38;5;66;03m# if str-dtype, convert\u001b[39;00m\n\u001b[0;32m   2396\u001b[0m data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(data, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mobject_)\n\u001b[1;32m-> 2398\u001b[0m result, tz_parsed \u001b[38;5;241m=\u001b[39m \u001b[43mtslib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray_to_datetime\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2399\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2400\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2401\u001b[0m \u001b[43m    \u001b[49m\u001b[43mutc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mutc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdayfirst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdayfirst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2403\u001b[0m \u001b[43m    \u001b[49m\u001b[43myearfirst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43myearfirst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreso\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mabbrev_to_npy_unit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_unit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2405\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tz_parsed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2408\u001b[0m     \u001b[38;5;66;03m# We can take a shortcut since the datetime64 numpy array\u001b[39;00m\n\u001b[0;32m   2409\u001b[0m     \u001b[38;5;66;03m#  is in UTC\u001b[39;00m\n\u001b[0;32m   2410\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result, tz_parsed\n",
      "File \u001b[1;32mtslib.pyx:414\u001b[0m, in \u001b[0;36mpandas._libs.tslib.array_to_datetime\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mtslib.pyx:596\u001b[0m, in \u001b[0;36mpandas._libs.tslib.array_to_datetime\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mtslib.pyx:553\u001b[0m, in \u001b[0;36mpandas._libs.tslib.array_to_datetime\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mconversion.pyx:641\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.conversion.convert_str_to_tsobject\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsing.pyx:336\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.parsing.parse_datetime_string\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsing.pyx:666\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.parsing.dateutil_parse\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mDateParseError\u001b[0m: Unknown datetime string format, unable to parse: 27-MAR-1987 00:03:35.68&#5;&#5;&#5;F, at position 4858"
     ]
    }
   ],
   "source": [
    "\n",
    "train_df_text,train_additional_features,train_labels,label_encoder = preprocessing(df_train,topic_column,topics,topics_to_remove,list_columns,drop_columns,date_columns,notnan_columns,text_columns,label_encoder)\n",
    "\n",
    "test_df_text,test_additional_features,test_labels,label_encoder = preprocessing(df_test,topic_column,topics,topics_to_remove,list_columns,drop_columns,date_columns,notnan_columns,text_columns,label_encoder,fit_encoder=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2506 entries, 1 to 3298\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    2506 non-null   object\n",
      " 1   title   2506 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 58.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df_text.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\theinrich\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\theinrich\\.cache\\huggingface\\hub\\models--bert-base-multilingual-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
    "\n",
    "def tokenize_texts(text):\n",
    "    tokenized_text = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "    return tokenized_text\n",
    "tokenized_inputs1 = []\n",
    "tokenized_inputs2 = []\n",
    "\n",
    "for idx, row in df_text.iterrows():\n",
    "    inputs1 = tokenize_texts(row['text'])\n",
    "    inputs2 = tokenize_texts(row['title'])\n",
    "    tokenized_inputs1.append(inputs1)\n",
    "    tokenized_inputs2.append(inputs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Text_Text_Feature_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokenized_inputs1, tokenized_inputs2, additional_features, labels):\n",
    "        self.tokenized_inputs1 = tokenized_inputs1\n",
    "        self.tokenized_inputs2 = tokenized_inputs2\n",
    "        self.additional_features = additional_features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids1 = self.tokenized_inputs1[idx]['input_ids'].squeeze()\n",
    "        attention_mask1 = self.tokenized_inputs1[idx]['attention_mask'].squeeze()\n",
    "        input_ids2 = self.tokenized_inputs2[idx]['input_ids'].squeeze()\n",
    "        attention_mask2 = self.tokenized_inputs2[idx]['attention_mask'].squeeze()\n",
    "        additional_features = self.additional_features[idx]\n",
    "        label = self.labels[idx]\n",
    "        return input_ids1, attention_mask1, input_ids2, attention_mask2, additional_features, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Text_Text_Feature_Dataset(tokenized_inputs1, tokenized_inputs2, additional_features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilingualBERTClassifier(nn.Module):\n",
    "    def __init__(self, bert_model_name='bert-base-multilingual-uncased', num_additional_features=10, num_classes=2):\n",
    "        super(MultilingualBERTClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        self.additional_features_layer = nn.Linear(num_additional_features, 128)\n",
    "        self.classifier = nn.Linear(768 * 2 + 128, num_classes)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, input_ids1, attention_mask1, input_ids2, attention_mask2, additional_features):\n",
    "        outputs1 = self.bert(input_ids1, attention_mask=attention_mask1)\n",
    "        pooled_output1 = outputs1[1]  # [CLS] token representation\n",
    "        \n",
    "        outputs2 = self.bert(input_ids2, attention_mask=attention_mask2)\n",
    "        pooled_output2 = outputs2[1]  # [CLS] token representation\n",
    "        \n",
    "        additional_features_output = self.additional_features_layer(additional_features)\n",
    "        additional_features_output = torch.relu(additional_features_output)\n",
    "        \n",
    "        combined_output = torch.cat((pooled_output1, pooled_output2, additional_features_output), dim=1)\n",
    "        combined_output = self.dropout(combined_output)\n",
    "        \n",
    "        logits = self.classifier(combined_output)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Beispiel Initialisierung\n",
    "num_additional_features = 10  # Anzahl der zus채tzlichen Features\n",
    "num_classes = 5  # Anzahl der Klassen (z.B. bin채re Klassifikation)\n",
    "\n",
    "model = MultilingualBERTClassifier(num_additional_features=num_additional_features, num_classes=num_classes)\n",
    "\n",
    "# Tokenizer initialisieren\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "# Beispiel Eingabedaten\n",
    "text1 = \"Dies ist ein Beispielsatz.\"\n",
    "text2 = \"Dies ist ein anderer Beispielsatz.\"\n",
    "additional_features = torch.randn(1, num_additional_features)  # Dummy-Daten f체r zus채tzliche Features\n",
    "\n",
    "# Tokenisieren\n",
    "inputs1 = tokenizer(text1, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "inputs2 = tokenizer(text2, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "\n",
    "# Vorhersage\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(inputs1['input_ids'], inputs1['attention_mask'], inputs2['input_ids'], inputs2['attention_mask'], additional_features)\n",
    "    predictions = torch.softmax(logits, dim=1)\n",
    "\n",
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
