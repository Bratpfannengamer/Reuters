{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\theinrich\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "import joblib\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=pd.read_csv('ModApte_train.csv')\n",
    "df_test=pd.read_csv('ModApte_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_to_list(df,column_name):\n",
    "    result=df[column_name].replace({' ':''},regex=True)\n",
    "    result.replace({'\\\\n':''},regex=True,inplace=True)\n",
    "    result.replace({'\\'\\'':'\\',\\''},regex=True,inplace=True)\n",
    "    return result.apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9603 entries, 0 to 9602\n",
      "Data columns (total 13 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   text         8816 non-null   object\n",
      " 1   text_type    9603 non-null   object\n",
      " 2   topics       9603 non-null   object\n",
      " 3   lewis_split  9603 non-null   object\n",
      " 4   cgis_split   9603 non-null   object\n",
      " 5   old_id       9603 non-null   object\n",
      " 6   new_id       9603 non-null   object\n",
      " 7   places       9603 non-null   object\n",
      " 8   people       9603 non-null   object\n",
      " 9   orgs         9603 non-null   object\n",
      " 10  exchanges    9603 non-null   object\n",
      " 11  date         9603 non-null   object\n",
      " 12  title        9549 non-null   object\n",
      "dtypes: object(13)\n",
      "memory usage: 975.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text           object\n",
       "text_type      object\n",
       "topics         object\n",
       "lewis_split    object\n",
       "cgis_split     object\n",
       "old_id         object\n",
       "new_id         object\n",
       "places         object\n",
       "people         object\n",
       "orgs           object\n",
       "exchanges      object\n",
       "date           object\n",
       "title          object\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_type</th>\n",
       "      <th>topics</th>\n",
       "      <th>lewis_split</th>\n",
       "      <th>cgis_split</th>\n",
       "      <th>old_id</th>\n",
       "      <th>new_id</th>\n",
       "      <th>places</th>\n",
       "      <th>people</th>\n",
       "      <th>orgs</th>\n",
       "      <th>exchanges</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Showers continued throughout the week in\\nthe ...</td>\n",
       "      <td>\"NORM\"</td>\n",
       "      <td>['cocoa']</td>\n",
       "      <td>\"TRAIN\"</td>\n",
       "      <td>\"TRAINING-SET\"</td>\n",
       "      <td>\"5544\"</td>\n",
       "      <td>\"1\"</td>\n",
       "      <td>['el-salvador' 'usa' 'uruguay']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>26-FEB-1987 15:01:01.79</td>\n",
       "      <td>BAHIA COCOA REVIEW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The U.S. Agriculture Department\\nreported the ...</td>\n",
       "      <td>\"NORM\"</td>\n",
       "      <td>['grain' 'wheat' 'corn' 'barley' 'oat' 'sorghum']</td>\n",
       "      <td>\"TRAIN\"</td>\n",
       "      <td>\"TRAINING-SET\"</td>\n",
       "      <td>\"5548\"</td>\n",
       "      <td>\"5\"</td>\n",
       "      <td>['usa']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>26-FEB-1987 15:10:44.60</td>\n",
       "      <td>NATIONAL AVERAGE PRICES FOR FARMER-OWNED RESERVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Argentine grain board figures show\\ncrop regis...</td>\n",
       "      <td>\"NORM\"</td>\n",
       "      <td>['veg-oil' 'linseed' 'lin-oil' 'soy-oil' 'sun-...</td>\n",
       "      <td>\"TRAIN\"</td>\n",
       "      <td>\"TRAINING-SET\"</td>\n",
       "      <td>\"5549\"</td>\n",
       "      <td>\"6\"</td>\n",
       "      <td>['argentina']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>26-FEB-1987 15:14:36.41</td>\n",
       "      <td>ARGENTINE 1986/87 GRAIN/OILSEED REGISTRATIONS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Moody's Investors Service Inc said it\\nlowered...</td>\n",
       "      <td>\"NORM\"</td>\n",
       "      <td>[]</td>\n",
       "      <td>\"TRAIN\"</td>\n",
       "      <td>\"TRAINING-SET\"</td>\n",
       "      <td>\"5551\"</td>\n",
       "      <td>\"8\"</td>\n",
       "      <td>['usa']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>26-FEB-1987 15:15:40.12</td>\n",
       "      <td>USX &amp;lt;X&gt; DEBT DOWGRADED BY MOODY'S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Champion Products Inc said its\\nboard of direc...</td>\n",
       "      <td>\"NORM\"</td>\n",
       "      <td>['earn']</td>\n",
       "      <td>\"TRAIN\"</td>\n",
       "      <td>\"TRAINING-SET\"</td>\n",
       "      <td>\"5552\"</td>\n",
       "      <td>\"9\"</td>\n",
       "      <td>['usa']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>26-FEB-1987 15:17:11.20</td>\n",
       "      <td>CHAMPION PRODUCTS &amp;lt;CH&gt; APPROVES STOCK SPLIT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text text_type  \\\n",
       "0  Showers continued throughout the week in\\nthe ...    \"NORM\"   \n",
       "1  The U.S. Agriculture Department\\nreported the ...    \"NORM\"   \n",
       "2  Argentine grain board figures show\\ncrop regis...    \"NORM\"   \n",
       "3  Moody's Investors Service Inc said it\\nlowered...    \"NORM\"   \n",
       "4  Champion Products Inc said its\\nboard of direc...    \"NORM\"   \n",
       "\n",
       "                                              topics lewis_split  \\\n",
       "0                                          ['cocoa']     \"TRAIN\"   \n",
       "1  ['grain' 'wheat' 'corn' 'barley' 'oat' 'sorghum']     \"TRAIN\"   \n",
       "2  ['veg-oil' 'linseed' 'lin-oil' 'soy-oil' 'sun-...     \"TRAIN\"   \n",
       "3                                                 []     \"TRAIN\"   \n",
       "4                                           ['earn']     \"TRAIN\"   \n",
       "\n",
       "       cgis_split  old_id new_id                           places people orgs  \\\n",
       "0  \"TRAINING-SET\"  \"5544\"    \"1\"  ['el-salvador' 'usa' 'uruguay']     []   []   \n",
       "1  \"TRAINING-SET\"  \"5548\"    \"5\"                          ['usa']     []   []   \n",
       "2  \"TRAINING-SET\"  \"5549\"    \"6\"                    ['argentina']     []   []   \n",
       "3  \"TRAINING-SET\"  \"5551\"    \"8\"                          ['usa']     []   []   \n",
       "4  \"TRAINING-SET\"  \"5552\"    \"9\"                          ['usa']     []   []   \n",
       "\n",
       "  exchanges                     date  \\\n",
       "0        []  26-FEB-1987 15:01:01.79   \n",
       "1        []  26-FEB-1987 15:10:44.60   \n",
       "2        []  26-FEB-1987 15:14:36.41   \n",
       "3        []  26-FEB-1987 15:15:40.12   \n",
       "4        []  26-FEB-1987 15:17:11.20   \n",
       "\n",
       "                                              title  \n",
       "0                                BAHIA COCOA REVIEW  \n",
       "1  NATIONAL AVERAGE PRICES FOR FARMER-OWNED RESERVE  \n",
       "2     ARGENTINE 1986/87 GRAIN/OILSEED REGISTRATIONS  \n",
       "3              USX &lt;X> DEBT DOWGRADED BY MOODY'S  \n",
       "4    CHAMPION PRODUCTS &lt;CH> APPROVES STOCK SPLIT  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_encoder=True\n",
    "fit_encoder=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define treatment of columns und topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define topics\n",
    "topic_column = 'topics'\n",
    "food = ['coconut', 'cotton-oil', 'sorghum', 'orange', 'rice', 'soybean', 'sun-meal', \n",
    "    'oilseed', 'sugar', 'hog', 'coffee', 'groundnut', 'sunseed', 'sun-oil', 'rye', \n",
    "    'lin-oil', 'copra-cake', 'potato', 'barley', 'tea', 'meal-feed', 'coconut-oil', \n",
    "    'palmkernel', 'cottonseed', 'castor-oil', 'l-cattle', 'livestock', 'soy-oil', \n",
    "    'rape-oil', 'palm-oil', 'cocoa', 'cotton', 'wheat', 'corn', 'f-cattle', 'grain', \n",
    "    'soy-meal', 'oat', 'groundnut-oil', 'veg-oil','rapeseed']\n",
    "resource = ['platinum', 'lead', 'nickel', 'strategic-metal', 'copper', 'palladium', 'gold', \n",
    "    'zinc', 'tin', 'iron-steel', 'alum', 'silver', 'nat-gas', 'rubber', 'pet-chem', 'fuel', 'crude','lumber','propane']\n",
    "finance = ['money-supply', 'dlr', 'nkr', 'lei', 'yen', 'dfl', 'sfr', 'cpi', 'instal-debt', \n",
    "    'money-fx', 'gnp', 'interest', 'income', 'dmk', 'rand', 'bop', 'reserves', 'nzdlr','acq']\n",
    "personal_finance = ['housing','jobs','earn']\n",
    "transport = ['jet', 'ship']\n",
    "topics=[[food,'food'],[resource,'resource'],[finance,'finance'],[personal_finance,'personal_finance'],[transport,'transport']]\n",
    "topics_to_remove = ['gas', 'heat', 'trade', 'retail', 'carcass', 'cpu', 'wpi', 'naphtha', 'ipi']\n",
    "\n",
    "#columns with special treatment\n",
    "list_columns=['places']\n",
    "drop_columns=['text_type','people','orgs','exchanges','lewis_split','cgis_split','old_id','new_id']\n",
    "notnan_columns=['text','topics']\n",
    "date_columns=['date']\n",
    "text_columns=['text','title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define functions for Prepeocessing\n",
    "\n",
    "These may be turned into a library later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_row_notnan_columms(df,notnan_columns):\n",
    "    for column in notnan_columns:\n",
    "        df[column].dropna(inplace=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funtion to reorganize a column of subtopics into  broader topics and removing some of them \n",
    "def categorize_topics(df,column,topics,remove):\n",
    "    for topic in topics:\n",
    "        for subtopic in topic[0]:\n",
    "            df[column]=df[column].replace({'\\''+subtopic+'\\'':'\\''+topic[1] +'\\''},regex=True)\n",
    "    for subtopic in remove:\n",
    "        df[column]=df[column].replace({'\\''+subtopic+'\\'':''},regex=True)\n",
    "    df[column]=df[column].replace({' ':''},regex=True)\n",
    "    df[column]=series_to_list(df,column)\n",
    "    df=df[df[column].str.len()==1]\n",
    "    df[column]=df[column].apply(lambda x : x[0])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explode_listcolumns(df,list_columns):\n",
    "    for column in list_columns:\n",
    "        df[column]=series_to_list(df,column)\n",
    "        df = df.join(pd.crosstab((s:=df[column].explode()).index, s).add_prefix(column+'_'))\n",
    "        df = df.drop(columns=column)\n",
    "        for col  in [col for col in df if col.startswith(column+'_')]:\n",
    "            df[col].fillna(value=0,inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_datecolumns(df,date_columns):\n",
    "    for column in date_columns:\n",
    "        # Die Zeichenkette in ein Datum konvertieren\n",
    "        df[column] = pd.to_datetime(df[column].str.strip().str.split(' ').str.get(0))\n",
    "        df[column+'_month'] = df[column].dt.month\n",
    "        df[column+'_month'] = (df[column+'_month'] - df[column+'_month'].mean()) / df[column+'_month'].std()\n",
    "\n",
    "        # Woche extrahieren (altes Verhalten, ab Pandas 1.1.0 ist isocalendar().week empfohlen)\n",
    "        df[column+'_day_month'] = df[column].dt.day\n",
    "        df[column+'_day_month'] = (df[column+'_day_month'] - df[column+'_day_month'].mean()) / df[column+'_day_month'].std()\n",
    "\n",
    "        # Tag extrahieren\n",
    "        df[column+'_day_year'] = df[column].dt.day_of_year\n",
    "        df[column+'_day_year'] = (df[column+'_day_year'] - df[column+'_day_year'].mean()) / df[column+'_day_year'].std()\n",
    "\n",
    "\n",
    "        # Wochentag extrahieren (Montag=0, Sonntag=6)\n",
    "        df[column+'_weekday'] = df[column].dt.day_name('en')\n",
    "\n",
    "        df[column+'_quarter_year'] = df[column].dt.quarter\n",
    "        df=pd.get_dummies(df, columns=[column+'_weekday'])\n",
    "        weekdays=['weekday_Monday','weekday_Tuesday','weekday_Wednesday','weekday_Thursday','weekday_Friday','weekday_Saturday','weekday_Sunday']\n",
    "        for weekday in weekdays:\n",
    "            if not column+'_'+weekday in df.columns:\n",
    "                df[column+'_'+weekday]=0\n",
    "            else:\n",
    "                df[column+'_'+weekday]=df[column+'_'+weekday].astype(int)\n",
    "        \n",
    "        df=df.drop(columns=column)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_textcolumns(df,text_columns):\n",
    "    for column in text_columns:\n",
    "        df[column].replace({'&lt;':'<'},regex=True,inplace=True)\n",
    "        df[column].replace({'\\\\n':' '},regex=True,inplace=True)\n",
    "        df[column]=df[column].str.replace('\\s+', ' ', regex=True)\n",
    "        df[column]=df[column].str.lower()\n",
    "        df[column]=df[column].fillna(value='')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_special_columns(df,list_columns,drop_columns,date_columns,notnan_columns,text_columns):\n",
    "    df=df.drop(columns=drop_columns)\n",
    "    df= explode_listcolumns(df,list_columns)\n",
    "    df= format_datecolumns(df,date_columns)\n",
    "    df= drop_row_notnan_columms(df,notnan_columns)\n",
    "    df= format_textcolumns(df,text_columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#has to expanded to make it readeble by model\n",
    "def preprocessing(df,topic_column,topics,topics_to_remove,list_columns,drop_columns,date_columns,notnan_columns,text_columns,encoder,fit_encoder=True):\n",
    "    df=categorize_topics(df,topic_column,topics,topics_to_remove)\n",
    "    df = handle_special_columns(df,list_columns,drop_columns,date_columns,notnan_columns,text_columns)\n",
    "    \n",
    "    #create torch tensor \n",
    "    additional_features = torch.tensor(df.drop(columns=(text_columns+[topic_column])).values)\n",
    "\n",
    "    #labels\n",
    "    if fit_encoder:\n",
    "        labels = torch.tensor(encoder.fit_transform(df[topic_column]))\n",
    "    else:\n",
    "        labels = torch.tensor(encoder.transform(df[topic_column]))\n",
    "    return df[text_columns],additional_features,labels,encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actual Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_encoder:\n",
    "    label_encoder = joblib.load('label_encoder.joblib')\n",
    "else:\n",
    "   label_encoder = LabelEncoder() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\theinrich\\AppData\\Local\\Temp\\ipykernel_26192\\426969037.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column]=df[column].apply(lambda x : x[0])\n",
      "C:\\Users\\theinrich\\AppData\\Local\\Temp\\ipykernel_26192\\2598233951.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(value=0,inplace=True)\n",
      "C:\\Users\\theinrich\\AppData\\Local\\Temp\\ipykernel_26192\\1198253871.py:4: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[column] = pd.to_datetime(df[column].str.strip().str.split(' ').str.get(0))\n",
      "C:\\Users\\theinrich\\AppData\\Local\\Temp\\ipykernel_26192\\411314824.py:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[column].replace({'&lt;':'<'},regex=True,inplace=True)\n",
      "C:\\Users\\theinrich\\AppData\\Local\\Temp\\ipykernel_26192\\426969037.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column]=df[column].apply(lambda x : x[0])\n",
      "C:\\Users\\theinrich\\AppData\\Local\\Temp\\ipykernel_26192\\2598233951.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(value=0,inplace=True)\n",
      "C:\\Users\\theinrich\\AppData\\Local\\Temp\\ipykernel_26192\\1198253871.py:4: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[column] = pd.to_datetime(df[column].str.strip().str.split(' ').str.get(0))\n",
      "C:\\Users\\theinrich\\AppData\\Local\\Temp\\ipykernel_26192\\411314824.py:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[column].replace({'&lt;':'<'},regex=True,inplace=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_df_text,train_additional_features,train_labels,label_encoder = preprocessing(df_train,topic_column,topics,topics_to_remove,list_columns,drop_columns,date_columns,notnan_columns,text_columns,label_encoder,fit_encoder=fit_encoder)\n",
    "\n",
    "test_df_text,test_additional_features,test_labels,label_encoder = preprocessing(df_test,topic_column,topics,topics_to_remove,list_columns,drop_columns,date_columns,notnan_columns,text_columns,label_encoder,fit_encoder=False)\n",
    "\n",
    "if not load_encoder:\n",
    "    joblib.dump(label_encoder, 'label_encoder.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Erstellung des Modell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definieren der Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='bert-base-multilingual-uncased'\n",
    "num_additional_features=119\n",
    "num_classes=5\n",
    "freeze_bert=True\n",
    "num_epochs=5\n",
    "batch_size=16\n",
    "load_model=False\n",
    "model_path='model.pth'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definieren der benötigten Funktionen und Objekte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_texts(text,length=128):\n",
    "    tokenized_text = tokenizer(text, return_tensors='pt', padding='max_length', truncation=True, max_length=length)\n",
    "    return tokenized_text\n",
    "\n",
    "def tokenize_inputs(df_text):\n",
    "    tokenized_inputs1 = []\n",
    "    tokenized_inputs2 = []\n",
    "    for idx, row in df_text.iterrows():\n",
    "        inputs1 = tokenize_texts(row['text'],256)\n",
    "        inputs2 = tokenize_texts(row['title'],16)\n",
    "        tokenized_inputs1.append(inputs1)\n",
    "        tokenized_inputs2.append(inputs2)\n",
    "    return tokenized_inputs1,tokenized_inputs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Text_Text_Feature_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokenized_inputs1, tokenized_inputs2, additional_features, labels):\n",
    "        self.tokenized_inputs1 = tokenized_inputs1\n",
    "        self.tokenized_inputs2 = tokenized_inputs2\n",
    "        self.additional_features = additional_features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids1 = self.tokenized_inputs1[idx]['input_ids'].squeeze()\n",
    "        attention_mask1 = self.tokenized_inputs1[idx]['attention_mask'].squeeze()\n",
    "        input_ids2 = self.tokenized_inputs2[idx]['input_ids'].squeeze()\n",
    "        attention_mask2 = self.tokenized_inputs2[idx]['attention_mask'].squeeze()\n",
    "        additional_features = self.additional_features[idx]\n",
    "        label = self.labels[idx]\n",
    "        return input_ids1, attention_mask1, input_ids2, attention_mask2, additional_features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilingualBERTClassifier(nn.Module):\n",
    "    def __init__(self, bert_model_name='bert-base-multilingual-uncased', num_additional_features=119, num_classes=5, freeze_bert=True):\n",
    "        super(MultilingualBERTClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        \n",
    "        # Einfrieren der BERT-Gewichte, falls angegeben\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        self.additional_features_layer = nn.Linear(num_additional_features, 128)\n",
    "        self.classifier = nn.Linear(768 * 2 + 128, num_classes)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, input_ids1, attention_mask1, input_ids2, attention_mask2, additional_features):\n",
    "        outputs1 = self.bert(input_ids1, attention_mask=attention_mask1)\n",
    "        pooled_output1 = outputs1[1]  # [CLS] token representation\n",
    "        \n",
    "        outputs2 = self.bert(input_ids2, attention_mask=attention_mask2)\n",
    "        pooled_output2 = outputs2[1]  # [CLS] token representation\n",
    "        \n",
    "        additional_features_output = self.additional_features_layer(additional_features)\n",
    "        additional_features_output = torch.relu(additional_features_output)\n",
    "        \n",
    "        combined_output = torch.cat((pooled_output1, pooled_output2, additional_features_output), dim=1)\n",
    "        combined_output = self.dropout(combined_output)\n",
    "        \n",
    "        logits = self.classifier(combined_output)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialisieren von Modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenized_inputs1, train_tokenized_inputs2= tokenize_inputs(train_df_text)\n",
    "test_tokenized_inputs1, test_tokenized_inputs2= tokenize_inputs(test_df_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Text_Text_Feature_Dataset(train_tokenized_inputs1, train_tokenized_inputs2,train_additional_features,train_labels)\n",
    "test_dataset = Text_Text_Feature_Dataset(test_tokenized_inputs1, test_tokenized_inputs2,test_additional_features,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultilingualBERTClassifier(num_additional_features=num_additional_features, num_classes=num_classes)\n",
    "\n",
    "if load_model:\n",
    "    model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# Verlustfunktion und Optimierer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for batch in tqdm(dataloader, desc=f'Epoch {epoch + 1}'):\n",
    "        input_ids1, attention_mask1, input_ids2, attention_mask2, additional_features, labels = batch\n",
    "        input_ids1, attention_mask1 = input_ids1.to(device), attention_mask1.to(device)\n",
    "        input_ids2, attention_mask2 = input_ids2.to(device), attention_mask2.to(device)\n",
    "        additional_features, labels = additional_features.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Vorwärtsdurchlauf\n",
    "        logits = model(input_ids1, attention_mask1, input_ids2, attention_mask2, additional_features)\n",
    "        \n",
    "        # Verlust berechnen\n",
    "        loss = criterion(logits, labels)\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        # Rückwärtsdurchlauf und Optimierung\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1} (BERT eingefroren), Loss: {epoch_loss / len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving Modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modell speichern mit PyTorch\n",
    "torch.save(model.state_dict(), model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
