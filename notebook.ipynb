{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install scikit-learn\n",
    "# %pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "# %pip install tqdm\n",
    "# %pip install transformers\n",
    "#%pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tilll\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import ast\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "import My_Machine_Learning_Tools as mytools\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=pd.read_csv('ModApte_train.csv')\n",
    "df_test=pd.read_csv('ModApte_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_to_list(df,column_name):\n",
    "    result=df[column_name].replace({' ':''},regex=True)\n",
    "    result.replace({'\\\\n':''},regex=True,inplace=True)\n",
    "    result.replace({'\\'\\'':'\\',\\''},regex=True,inplace=True)\n",
    "    return result.apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9603 entries, 0 to 9602\n",
      "Data columns (total 13 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   text         8816 non-null   object\n",
      " 1   text_type    9603 non-null   object\n",
      " 2   topics       9603 non-null   object\n",
      " 3   lewis_split  9603 non-null   object\n",
      " 4   cgis_split   9603 non-null   object\n",
      " 5   old_id       9603 non-null   object\n",
      " 6   new_id       9603 non-null   object\n",
      " 7   places       9603 non-null   object\n",
      " 8   people       9603 non-null   object\n",
      " 9   orgs         9603 non-null   object\n",
      " 10  exchanges    9603 non-null   object\n",
      " 11  date         9603 non-null   object\n",
      " 12  title        9549 non-null   object\n",
      "dtypes: object(13)\n",
      "memory usage: 975.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text           object\n",
       "text_type      object\n",
       "topics         object\n",
       "lewis_split    object\n",
       "cgis_split     object\n",
       "old_id         object\n",
       "new_id         object\n",
       "places         object\n",
       "people         object\n",
       "orgs           object\n",
       "exchanges      object\n",
       "date           object\n",
       "title          object\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_type</th>\n",
       "      <th>topics</th>\n",
       "      <th>lewis_split</th>\n",
       "      <th>cgis_split</th>\n",
       "      <th>old_id</th>\n",
       "      <th>new_id</th>\n",
       "      <th>places</th>\n",
       "      <th>people</th>\n",
       "      <th>orgs</th>\n",
       "      <th>exchanges</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Showers continued throughout the week in\\nthe ...</td>\n",
       "      <td>\"NORM\"</td>\n",
       "      <td>['cocoa']</td>\n",
       "      <td>\"TRAIN\"</td>\n",
       "      <td>\"TRAINING-SET\"</td>\n",
       "      <td>\"5544\"</td>\n",
       "      <td>\"1\"</td>\n",
       "      <td>['el-salvador' 'usa' 'uruguay']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>26-FEB-1987 15:01:01.79</td>\n",
       "      <td>BAHIA COCOA REVIEW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The U.S. Agriculture Department\\nreported the ...</td>\n",
       "      <td>\"NORM\"</td>\n",
       "      <td>['grain' 'wheat' 'corn' 'barley' 'oat' 'sorghum']</td>\n",
       "      <td>\"TRAIN\"</td>\n",
       "      <td>\"TRAINING-SET\"</td>\n",
       "      <td>\"5548\"</td>\n",
       "      <td>\"5\"</td>\n",
       "      <td>['usa']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>26-FEB-1987 15:10:44.60</td>\n",
       "      <td>NATIONAL AVERAGE PRICES FOR FARMER-OWNED RESERVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Argentine grain board figures show\\ncrop regis...</td>\n",
       "      <td>\"NORM\"</td>\n",
       "      <td>['veg-oil' 'linseed' 'lin-oil' 'soy-oil' 'sun-...</td>\n",
       "      <td>\"TRAIN\"</td>\n",
       "      <td>\"TRAINING-SET\"</td>\n",
       "      <td>\"5549\"</td>\n",
       "      <td>\"6\"</td>\n",
       "      <td>['argentina']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>26-FEB-1987 15:14:36.41</td>\n",
       "      <td>ARGENTINE 1986/87 GRAIN/OILSEED REGISTRATIONS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Moody's Investors Service Inc said it\\nlowered...</td>\n",
       "      <td>\"NORM\"</td>\n",
       "      <td>[]</td>\n",
       "      <td>\"TRAIN\"</td>\n",
       "      <td>\"TRAINING-SET\"</td>\n",
       "      <td>\"5551\"</td>\n",
       "      <td>\"8\"</td>\n",
       "      <td>['usa']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>26-FEB-1987 15:15:40.12</td>\n",
       "      <td>USX &amp;lt;X&gt; DEBT DOWGRADED BY MOODY'S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Champion Products Inc said its\\nboard of direc...</td>\n",
       "      <td>\"NORM\"</td>\n",
       "      <td>['earn']</td>\n",
       "      <td>\"TRAIN\"</td>\n",
       "      <td>\"TRAINING-SET\"</td>\n",
       "      <td>\"5552\"</td>\n",
       "      <td>\"9\"</td>\n",
       "      <td>['usa']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>26-FEB-1987 15:17:11.20</td>\n",
       "      <td>CHAMPION PRODUCTS &amp;lt;CH&gt; APPROVES STOCK SPLIT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text text_type  \\\n",
       "0  Showers continued throughout the week in\\nthe ...    \"NORM\"   \n",
       "1  The U.S. Agriculture Department\\nreported the ...    \"NORM\"   \n",
       "2  Argentine grain board figures show\\ncrop regis...    \"NORM\"   \n",
       "3  Moody's Investors Service Inc said it\\nlowered...    \"NORM\"   \n",
       "4  Champion Products Inc said its\\nboard of direc...    \"NORM\"   \n",
       "\n",
       "                                              topics lewis_split  \\\n",
       "0                                          ['cocoa']     \"TRAIN\"   \n",
       "1  ['grain' 'wheat' 'corn' 'barley' 'oat' 'sorghum']     \"TRAIN\"   \n",
       "2  ['veg-oil' 'linseed' 'lin-oil' 'soy-oil' 'sun-...     \"TRAIN\"   \n",
       "3                                                 []     \"TRAIN\"   \n",
       "4                                           ['earn']     \"TRAIN\"   \n",
       "\n",
       "       cgis_split  old_id new_id                           places people orgs  \\\n",
       "0  \"TRAINING-SET\"  \"5544\"    \"1\"  ['el-salvador' 'usa' 'uruguay']     []   []   \n",
       "1  \"TRAINING-SET\"  \"5548\"    \"5\"                          ['usa']     []   []   \n",
       "2  \"TRAINING-SET\"  \"5549\"    \"6\"                    ['argentina']     []   []   \n",
       "3  \"TRAINING-SET\"  \"5551\"    \"8\"                          ['usa']     []   []   \n",
       "4  \"TRAINING-SET\"  \"5552\"    \"9\"                          ['usa']     []   []   \n",
       "\n",
       "  exchanges                     date  \\\n",
       "0        []  26-FEB-1987 15:01:01.79   \n",
       "1        []  26-FEB-1987 15:10:44.60   \n",
       "2        []  26-FEB-1987 15:14:36.41   \n",
       "3        []  26-FEB-1987 15:15:40.12   \n",
       "4        []  26-FEB-1987 15:17:11.20   \n",
       "\n",
       "                                              title  \n",
       "0                                BAHIA COCOA REVIEW  \n",
       "1  NATIONAL AVERAGE PRICES FOR FARMER-OWNED RESERVE  \n",
       "2     ARGENTINE 1986/87 GRAIN/OILSEED REGISTRATIONS  \n",
       "3              USX &lt;X> DEBT DOWGRADED BY MOODY'S  \n",
       "4    CHAMPION PRODUCTS &lt;CH> APPROVES STOCK SPLIT  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_encoder=False\n",
    "fit_encoder=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define treatment of columns und topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define topics\n",
    "topic_column = 'topics'\n",
    "food = ['coconut', 'cotton-oil', 'sorghum', 'orange', 'rice', 'soybean', 'sun-meal', \n",
    "    'oilseed', 'sugar', 'hog', 'coffee', 'groundnut', 'sunseed', 'sun-oil', 'rye', \n",
    "    'lin-oil', 'copra-cake', 'potato', 'barley', 'tea', 'meal-feed', 'coconut-oil', \n",
    "    'palmkernel', 'cottonseed', 'castor-oil', 'l-cattle', 'livestock', 'soy-oil', \n",
    "    'rape-oil', 'palm-oil', 'cocoa', 'cotton', 'wheat', 'corn', 'f-cattle', 'grain', \n",
    "    'soy-meal', 'oat', 'groundnut-oil', 'veg-oil','rapeseed']\n",
    "resource = ['platinum', 'lead', 'nickel', 'strategic-metal', 'copper', 'palladium', 'gold', \n",
    "    'zinc', 'tin', 'iron-steel', 'alum', 'silver', 'nat-gas', 'rubber', 'pet-chem', 'fuel', 'crude','lumber','propane','wool']\n",
    "finance = ['money-supply', 'dlr', 'nkr', 'lei', 'yen', 'dfl', 'sfr', 'cpi', 'instal-debt', \n",
    "    'money-fx', 'gnp', 'interest', 'income', 'dmk', 'rand', 'bop', 'reserves', 'nzdlr','acq']\n",
    "personal_finance = ['housing','jobs','earn']\n",
    "transport = ['jet', 'ship']\n",
    "topics=[[food,'food'],[resource,'resource'],[finance,'finance'],[personal_finance,'personal_finance'],[transport,'transport']]\n",
    "topics_to_remove = ['gas', 'heat', 'trade', 'retail', 'carcass', 'cpu', 'wpi', 'naphtha', 'ipi','stg','inventories']\n",
    "\n",
    "#columns with special treatment\n",
    "list_column='places'\n",
    "drop_columns=['text_type','people','orgs','exchanges','lewis_split','cgis_split','old_id','new_id']\n",
    "notnan_columns=['text','topics']\n",
    "date_columns=['date']\n",
    "text_columns=['text','title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define functions for Prepeocessing\n",
    "\n",
    "These may be turned into a library later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_row_notnan_columms(df,notnan_columns):\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    for column in notnan_columns:\n",
    "        df_copy[column].dropna(inplace=True)\n",
    "    \n",
    "    return df_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_listcolumns(df, column):\n",
    "    \"\"\"\n",
    "    Wandelt eine Spalte mit Listen als Strings formatiert in echte Listen um und gibt ein DataFrame und die eindeutigen Werte zurück.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Der DataFrame, der die Spalte enthält.\n",
    "    column (str): Der Name der Spalte, die konvertiert werden soll.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Das DataFrame mit der umgewandelten Spalte.\n",
    "    list: Eine Liste der eindeutigen Werte in der umgewandelten Spalte.\n",
    "\n",
    "    Example:\n",
    "    >>> df, unique_values = format_listcolumns(df_train, 'features')\n",
    "    \"\"\"\n",
    "    # Kopie der Spalte erstellen, um die Originaldaten nicht zu ändern\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # Umwandlung der Spalte von einem String in eine Liste\n",
    "    df_copy[column].replace({'\\\\n': ''}, regex=True, inplace=True)\n",
    "    df_copy = mytools.df_string_to_list(df_copy, column, entry_delimiter=\"'\", separator=' ')\n",
    "\n",
    "    # Eindeutige Werte in der umgewandelten Spalte finden\n",
    "    unique_values = mytools.df_unique_list_values(df_copy, column)\n",
    "\n",
    "    return df_copy, unique_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funtion to reorganize a column of subtopics into  broader topics and removing some of them \n",
    "def categorize_topics(df,column,topics,remove):\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    for topic in topics:\n",
    "        for subtopic in topic[0]:\n",
    "            df_copy[column] = df_copy[column].replace({'\\'' + subtopic + '\\'': '\\'' + topic[1] + '\\''}, regex=True)\n",
    "    \n",
    "    for subtopic in remove:\n",
    "        df_copy[column] = df_copy[column].replace({'\\'' + subtopic + '\\'': ''}, regex=True)\n",
    "    \n",
    "    df_copy[column] = df_copy[column].replace({' ': ''}, regex=True)\n",
    "    df_copy[column] = series_to_list(df_copy, column)\n",
    "    df_copy = df_copy[df_copy[column].str.len() == 1]\n",
    "    df_copy[column] = df_copy[column].apply(lambda x: x[0])\n",
    "    \n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_datecolumns(df,date_columns):\n",
    "    # Kopie des DataFrame erstellen, um die Originaldaten nicht zu ändern\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    for column in date_columns:\n",
    "        # Die Zeichenkette in ein Datum konvertieren\n",
    "        df_copy[column] = pd.to_datetime(df_copy[column].str.strip().str.split(' ').str.get(0))\n",
    "        df_copy[column+'_month'] = df_copy[column].dt.month\n",
    "        df_copy[column+'_month'] = (df_copy[column+'_month'] - df_copy[column+'_month'].mean()) / df_copy[column+'_month'].std()\n",
    "\n",
    "        # Woche extrahieren (altes Verhalten, ab Pandas 1.1.0 ist isocalendar().week empfohlen)\n",
    "        df_copy[column+'_day_month'] = df_copy[column].dt.day\n",
    "        df_copy[column+'_day_month'] = (df_copy[column+'_day_month'] - df_copy[column+'_day_month'].mean()) / df_copy[column+'_day_month'].std()\n",
    "\n",
    "        # Tag extrahieren\n",
    "        df_copy[column+'_day_year'] = df_copy[column].dt.day_of_year\n",
    "        df_copy[column+'_day_year'] = (df_copy[column+'_day_year'] - df_copy[column+'_day_year'].mean()) / df_copy[column+'_day_year'].std()\n",
    "\n",
    "        # Wochentag extrahieren (Montag=0, Sonntag=6)\n",
    "        df_copy[column+'_weekday'] = df_copy[column].dt.day_name('en')\n",
    "\n",
    "        df_copy[column+'_quarter_year'] = df_copy[column].dt.quarter\n",
    "        df_copy = pd.get_dummies(df_copy, columns=[column+'_weekday'])\n",
    "        weekdays = ['weekday_Monday', 'weekday_Tuesday', 'weekday_Wednesday', 'weekday_Thursday', 'weekday_Friday', 'weekday_Saturday', 'weekday_Sunday']\n",
    "        for weekday in weekdays:\n",
    "            if not column+'_'+weekday in df_copy.columns:\n",
    "                df_copy[column+'_'+weekday] = 0\n",
    "            else:\n",
    "                df_copy[column+'_'+weekday] = df_copy[column+'_'+weekday].astype(int)\n",
    "\n",
    "        df_copy = df_copy.drop(columns=column)\n",
    "\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:8: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:8: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\tilll\\AppData\\Local\\Temp\\ipykernel_40936\\1029708371.py:8: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  df_copy[column] = df_copy[column].str.replace('\\s+', ' ', regex=True)\n"
     ]
    }
   ],
   "source": [
    "def format_textcolumns(df,text_columns):\n",
    "    # Kopie des DataFrame erstellen, um die Originaldaten nicht zu ändern\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    for column in text_columns:\n",
    "        df_copy[column].replace({'&lt;': '<'}, regex=True, inplace=True)\n",
    "        df_copy[column].replace({'\\\\n': ' '}, regex=True, inplace=True)\n",
    "        df_copy[column] = df_copy[column].str.replace('\\s+', ' ', regex=True)\n",
    "        df_copy[column] = df_copy[column].str.lower()\n",
    "        df_copy[column] = df_copy[column].fillna(value='')\n",
    "\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_special_columns(df,list_column,list_possible_values,drop_columns,date_columns,notnan_columns,text_columns):\n",
    "    # Kopie des DataFrame erstellen, um die Originaldaten nicht zu ändern\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # Spalten aus dem DataFrame entfernen\n",
    "    df_copy = df_copy.drop(columns=drop_columns)\n",
    "\n",
    "    # Spalte mit Listen explodieren und mögliche Werte festlegen\n",
    "    df_copy = mytools.df_explode_listcolumn(df_copy, list_column, list_possible_values)\n",
    "\n",
    "    # Datumsangaben formatieren\n",
    "    df_copy = format_datecolumns(df_copy, date_columns)\n",
    "\n",
    "    # Zeilen entfernen, die NaN-Werte in bestimmten Spalten enthalten\n",
    "    df_copy = drop_row_notnan_columms(df_copy, notnan_columns)\n",
    "\n",
    "    # Textspalten formatieren\n",
    "    df_copy = format_textcolumns(df_copy, text_columns)\n",
    "\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#has to expanded to make it readeble by model\n",
    "def preprocessing(df,topic_column,topics,topics_to_remove,list_column,list_possible_values,drop_columns,date_columns,notnan_columns,text_columns,encoder,fit_encoder=True):\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    df_copy = categorize_topics(df_copy, topic_column, topics, topics_to_remove)\n",
    "    df_copy = handle_special_columns(df_copy, list_column, list_possible_values, drop_columns, date_columns, notnan_columns, text_columns)\n",
    "    \n",
    "    additional_features = torch.tensor(df_copy.drop(columns=(text_columns + [topic_column])).values)\n",
    "    additional_features = additional_features.float()\n",
    "    \n",
    "    if fit_encoder:\n",
    "        labels = torch.tensor(encoder.fit_transform(df_copy[topic_column]))\n",
    "    else:\n",
    "        labels = torch.tensor(encoder.transform(df_copy[topic_column]))\n",
    "    labels = labels.long()\n",
    "    \n",
    "    return df_copy[text_columns], additional_features, labels, encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actual Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tilll\\AppData\\Local\\Temp\\ipykernel_40936\\2357064120.py:20: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_copy[column].replace({'\\\\n': ''}, regex=True, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "df,unique_values_test = format_listcolumns(df_test,list_column)\n",
    "df,unique_values_train = format_listcolumns(df_train,list_column)\n",
    "unique_countries=unique_values_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_encoder:\n",
    "    label_encoder = joblib.load('label_encoder.joblib')\n",
    "else:\n",
    "   label_encoder = LabelEncoder() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tilll\\AppData\\Local\\Temp\\ipykernel_40936\\4235866303.py:7: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_copy[column] = pd.to_datetime(df_copy[column].str.strip().str.split(' ').str.get(0))\n",
      "C:\\Users\\tilll\\AppData\\Local\\Temp\\ipykernel_40936\\1029708371.py:6: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_copy[column].replace({'&lt;': '<'}, regex=True, inplace=True)\n",
      "C:\\Users\\tilll\\AppData\\Local\\Temp\\ipykernel_40936\\4235866303.py:7: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_copy[column] = pd.to_datetime(df_copy[column].str.strip().str.split(' ').str.get(0))\n",
      "C:\\Users\\tilll\\AppData\\Local\\Temp\\ipykernel_40936\\1029708371.py:6: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_copy[column].replace({'&lt;': '<'}, regex=True, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_df_text,train_additional_features,train_labels,label_encoder = preprocessing(df_train,topic_column,topics,topics_to_remove,list_column,unique_countries,drop_columns,date_columns,notnan_columns,text_columns,label_encoder,fit_encoder=fit_encoder)\n",
    "\n",
    "test_df_text,test_additional_features,test_labels,label_encoder = preprocessing(df_test,topic_column,topics,topics_to_remove,list_column,unique_countries,drop_columns,date_columns,notnan_columns,text_columns,label_encoder,fit_encoder=fit_encoder)\n",
    "\n",
    "if not load_encoder:\n",
    "    joblib.dump(label_encoder, 'label_encoder.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Erstellung des Modell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definieren der Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='bert-base-multilingual-uncased'\n",
    "num_additional_features=139\n",
    "num_classes=5\n",
    "freeze_bert=True\n",
    "num_epochs=5\n",
    "batch_size=32\n",
    "model_path_base='models/Bert_freeze'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initilisieren vom Tokennizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definieren der benötigten Funktionen und Objekte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizen und Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_texts(text,length=128):\n",
    "    tokenized_text = tokenizer(text, return_tensors='pt', padding='max_length', truncation=True, max_length=length)\n",
    "    return tokenized_text\n",
    "\n",
    "def tokenize_inputs(df_text):\n",
    "    tokenized_inputs1 = []\n",
    "    tokenized_inputs2 = []\n",
    "    for idx, row in df_text.iterrows():\n",
    "        inputs1 = tokenize_texts(row['text'],256)\n",
    "        inputs2 = tokenize_texts(row['title'],16)\n",
    "        tokenized_inputs1.append(inputs1)\n",
    "        tokenized_inputs2.append(inputs2)\n",
    "    return tokenized_inputs1,tokenized_inputs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Text_Text_Feature_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokenized_inputs1, tokenized_inputs2, additional_features, labels):\n",
    "        self.tokenized_inputs1 = tokenized_inputs1\n",
    "        self.tokenized_inputs2 = tokenized_inputs2\n",
    "        self.additional_features = additional_features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids1 = self.tokenized_inputs1[idx]['input_ids'].squeeze()\n",
    "        attention_mask1 = self.tokenized_inputs1[idx]['attention_mask'].squeeze()\n",
    "        input_ids2 = self.tokenized_inputs2[idx]['input_ids'].squeeze()\n",
    "        attention_mask2 = self.tokenized_inputs2[idx]['attention_mask'].squeeze()\n",
    "        additional_features = self.additional_features[idx]\n",
    "        label = self.labels[idx]\n",
    "        return input_ids1, attention_mask1, input_ids2, attention_mask2, additional_features, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilingualBERTClassifier(nn.Module):\n",
    "    def __init__(self, bert_model_name='bert-base-multilingual-uncased', num_additional_features=119, num_classes=5, freeze_bert=True):\n",
    "        super(MultilingualBERTClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        \n",
    "        # Einfrieren der BERT-Gewichte, falls angegeben\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        self.additional_features_layer = nn.Linear(num_additional_features, 128)\n",
    "        self.classifier = nn.Linear(768 * 2 + 128, num_classes)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, input_ids1, attention_mask1, input_ids2, attention_mask2, additional_features):\n",
    "        outputs1 = self.bert(input_ids1, attention_mask=attention_mask1)\n",
    "        pooled_output1 = outputs1[1]  # [CLS] token representation\n",
    "        \n",
    "        outputs2 = self.bert(input_ids2, attention_mask=attention_mask2)\n",
    "        pooled_output2 = outputs2[1]  # [CLS] token representation\n",
    "        \n",
    "        additional_features_output = self.additional_features_layer(additional_features)\n",
    "        additional_features_output = torch.relu(additional_features_output)\n",
    "        \n",
    "        combined_output = torch.cat((pooled_output1, pooled_output2, additional_features_output), dim=1)\n",
    "        combined_output = self.dropout(combined_output)\n",
    "        \n",
    "        logits = self.classifier(combined_output)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Erstellen des Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize Text\n",
    "train_tokenized_inputs1, train_tokenized_inputs2= tokenize_inputs(train_df_text)\n",
    "test_tokenized_inputs1, test_tokenized_inputs2= tokenize_inputs(test_df_text)\n",
    "#Create Datasets\n",
    "train_dataset = Text_Text_Feature_Dataset(train_tokenized_inputs1, train_tokenized_inputs2,train_additional_features,train_labels)\n",
    "test_dataset = Text_Text_Feature_Dataset(test_tokenized_inputs1, test_tokenized_inputs2,test_additional_features,test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialisieren von Modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from models/Bert_freeze_v2.pt\n",
      "model loaded\n"
     ]
    }
   ],
   "source": [
    "#Create modell or load previous one\n",
    "model = MultilingualBERTClassifier(num_additional_features=num_additional_features, num_classes=num_classes)\n",
    "#load_model(model,model_path_base)\n",
    "mytools.modelversions_load_model(model,model_path_base)\n",
    "# move modell to divice if possible\n",
    "model.to(device)\n",
    "print('Model loaded and on device')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create Dataloader\n",
    "dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# Loss-Funktion and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 199/199 [00:59<00:00,  3.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 (BERT eingefroren), Loss: 1.1001562727755638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 199/199 [00:59<00:00,  3.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 (BERT eingefroren), Loss: 1.0979603894391852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 199/199 [01:01<00:00,  3.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 (BERT eingefroren), Loss: 1.0939032968564248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 199/199 [01:00<00:00,  3.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 (BERT eingefroren), Loss: 1.092449508420187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 199/199 [01:00<00:00,  3.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 (BERT eingefroren), Loss: 1.0834619058436485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for batch in tqdm(dataloader, desc=f'Epoch {epoch + 1}'):\n",
    "        input_ids1, attention_mask1, input_ids2, attention_mask2, additional_features, labels = batch\n",
    "        input_ids1, attention_mask1 = input_ids1.to(device), attention_mask1.to(device)\n",
    "        input_ids2, attention_mask2 = input_ids2.to(device), attention_mask2.to(device)\n",
    "        additional_features, labels = additional_features.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Vorwärtsdurchlauf\n",
    "        logits = model(input_ids1, attention_mask1, input_ids2, attention_mask2, additional_features)\n",
    "\n",
    "        # Verlust berechnen\n",
    "        loss = criterion(logits, labels)\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        # Rückwärtsdurchlauf und Optimierung\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1} (BERT eingefroren), Loss: {epoch_loss / len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:23<00:00,  3.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6532322426177175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#test modell\n",
    "model.eval()\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "test_predictions = []\n",
    "test_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_dataloader):\n",
    "        input_ids1, attention_mask1, input_ids2, attention_mask2, additional_features, labels = batch\n",
    "        input_ids1, attention_mask1 = input_ids1.to(device), attention_mask1.to(device)\n",
    "        input_ids2, attention_mask2 = input_ids2.to(device), attention_mask2.to(device)\n",
    "        additional_features, labels = additional_features.to(device), labels.to(device)\n",
    "\n",
    "        logits = model(input_ids1, attention_mask1, input_ids2, attention_mask2, additional_features)\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        \n",
    "        test_predictions.extend(predictions.cpu().numpy())\n",
    "        test_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "test_predictions = label_encoder.inverse_transform(test_predictions)\n",
    "test_labels = label_encoder.inverse_transform(test_labels)\n",
    "\n",
    "accuracy = sum(test_predictions == test_labels) / len(test_labels)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>finance</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>food</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>personal_finance</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resource</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>transport</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      0\n",
       "1                      \n",
       "finance            True\n",
       "food              False\n",
       "personal_finance   True\n",
       "resource          False\n",
       "transport         False"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define categories\n",
    "categories = ['food', 'resource', 'finance', 'personal_finance', 'transport']\n",
    "accuracy_data = [test_predictions==test_labels,test_labels]\n",
    "#total datapoints\n",
    "total = len(test_labels)\n",
    "# DataFrame erstellen\n",
    "accuracy_df = pd.DataFrame(accuracy_data).T\n",
    "#group by and sum for each category and total\n",
    "accuracy_df = accuracy_df.groupby(1).sum()\n",
    "accuracy_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Tabelle plotten\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39msubplots()\n\u001b[0;32m      3\u001b[0m ax\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtight\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m ax\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Tabelle plotten\n",
    "fig, ax = plt.subplots()\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "table = ax.table(cellText=accuracy_df.values, colLabels=accuracy_df.columns, rowLabels=accuracy_df.index, loc='center', cellLoc='center')\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(14)\n",
    "table.scale(1.2, 1.2)\n",
    "\n",
    "plt.title('Accuracy für verschiedene Kategorien')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for finance: 0.6567926455566905\n",
      "Accuracy for food: 0.0\n",
      "Accuracy for personal_finance: 0.9052823315118397\n",
      "Accuracy for resource: 0.0\n",
      "Accuracy for transport: 0.0\n"
     ]
    }
   ],
   "source": [
    "#print accuracy by topic\n",
    "for topic in label_encoder.classes_:\n",
    "    topic_mask = test_labels == topic\n",
    "    topic_accuracy = sum(test_predictions[topic_mask] == test_labels[topic_mask]) / sum(topic_mask)\n",
    "    print(f'Accuracy for {topic}: {topic_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving Modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as models/Bert_freeze_v2.pt\n"
     ]
    }
   ],
   "source": [
    "mytools.modelversions_save_model(model, model_path_base)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
