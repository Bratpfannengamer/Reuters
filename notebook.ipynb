{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install scikit-learn\n",
    "# %pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "# %pip install tqdm\n",
    "# %pip install transformers\n",
    "#%pip install matplotlib\n",
    "#%pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\theinrich\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import ast\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import My_Machine_Learning_Tools as mytools\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=pd.read_csv('ModApte_train.csv')\n",
    "df_test=pd.read_csv('ModApte_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_to_list(df,column_name):\n",
    "    result=df[column_name].replace({' ':''},regex=True)\n",
    "    result.replace({'\\\\n':''},regex=True,inplace=True)\n",
    "    result.replace({'\\'\\'':'\\',\\''},regex=True,inplace=True)\n",
    "    return result.apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9603 entries, 0 to 9602\n",
      "Data columns (total 13 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   text         8816 non-null   object\n",
      " 1   text_type    9603 non-null   object\n",
      " 2   topics       9603 non-null   object\n",
      " 3   lewis_split  9603 non-null   object\n",
      " 4   cgis_split   9603 non-null   object\n",
      " 5   old_id       9603 non-null   object\n",
      " 6   new_id       9603 non-null   object\n",
      " 7   places       9603 non-null   object\n",
      " 8   people       9603 non-null   object\n",
      " 9   orgs         9603 non-null   object\n",
      " 10  exchanges    9603 non-null   object\n",
      " 11  date         9603 non-null   object\n",
      " 12  title        9549 non-null   object\n",
      "dtypes: object(13)\n",
      "memory usage: 975.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text           object\n",
       "text_type      object\n",
       "topics         object\n",
       "lewis_split    object\n",
       "cgis_split     object\n",
       "old_id         object\n",
       "new_id         object\n",
       "places         object\n",
       "people         object\n",
       "orgs           object\n",
       "exchanges      object\n",
       "date           object\n",
       "title          object\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_type</th>\n",
       "      <th>topics</th>\n",
       "      <th>lewis_split</th>\n",
       "      <th>cgis_split</th>\n",
       "      <th>old_id</th>\n",
       "      <th>new_id</th>\n",
       "      <th>places</th>\n",
       "      <th>people</th>\n",
       "      <th>orgs</th>\n",
       "      <th>exchanges</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Showers continued throughout the week in\\nthe ...</td>\n",
       "      <td>\"NORM\"</td>\n",
       "      <td>['cocoa']</td>\n",
       "      <td>\"TRAIN\"</td>\n",
       "      <td>\"TRAINING-SET\"</td>\n",
       "      <td>\"5544\"</td>\n",
       "      <td>\"1\"</td>\n",
       "      <td>['el-salvador' 'usa' 'uruguay']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>26-FEB-1987 15:01:01.79</td>\n",
       "      <td>BAHIA COCOA REVIEW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The U.S. Agriculture Department\\nreported the ...</td>\n",
       "      <td>\"NORM\"</td>\n",
       "      <td>['grain' 'wheat' 'corn' 'barley' 'oat' 'sorghum']</td>\n",
       "      <td>\"TRAIN\"</td>\n",
       "      <td>\"TRAINING-SET\"</td>\n",
       "      <td>\"5548\"</td>\n",
       "      <td>\"5\"</td>\n",
       "      <td>['usa']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>26-FEB-1987 15:10:44.60</td>\n",
       "      <td>NATIONAL AVERAGE PRICES FOR FARMER-OWNED RESERVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Argentine grain board figures show\\ncrop regis...</td>\n",
       "      <td>\"NORM\"</td>\n",
       "      <td>['veg-oil' 'linseed' 'lin-oil' 'soy-oil' 'sun-...</td>\n",
       "      <td>\"TRAIN\"</td>\n",
       "      <td>\"TRAINING-SET\"</td>\n",
       "      <td>\"5549\"</td>\n",
       "      <td>\"6\"</td>\n",
       "      <td>['argentina']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>26-FEB-1987 15:14:36.41</td>\n",
       "      <td>ARGENTINE 1986/87 GRAIN/OILSEED REGISTRATIONS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Moody's Investors Service Inc said it\\nlowered...</td>\n",
       "      <td>\"NORM\"</td>\n",
       "      <td>[]</td>\n",
       "      <td>\"TRAIN\"</td>\n",
       "      <td>\"TRAINING-SET\"</td>\n",
       "      <td>\"5551\"</td>\n",
       "      <td>\"8\"</td>\n",
       "      <td>['usa']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>26-FEB-1987 15:15:40.12</td>\n",
       "      <td>USX &amp;lt;X&gt; DEBT DOWGRADED BY MOODY'S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Champion Products Inc said its\\nboard of direc...</td>\n",
       "      <td>\"NORM\"</td>\n",
       "      <td>['earn']</td>\n",
       "      <td>\"TRAIN\"</td>\n",
       "      <td>\"TRAINING-SET\"</td>\n",
       "      <td>\"5552\"</td>\n",
       "      <td>\"9\"</td>\n",
       "      <td>['usa']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>26-FEB-1987 15:17:11.20</td>\n",
       "      <td>CHAMPION PRODUCTS &amp;lt;CH&gt; APPROVES STOCK SPLIT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text text_type  \\\n",
       "0  Showers continued throughout the week in\\nthe ...    \"NORM\"   \n",
       "1  The U.S. Agriculture Department\\nreported the ...    \"NORM\"   \n",
       "2  Argentine grain board figures show\\ncrop regis...    \"NORM\"   \n",
       "3  Moody's Investors Service Inc said it\\nlowered...    \"NORM\"   \n",
       "4  Champion Products Inc said its\\nboard of direc...    \"NORM\"   \n",
       "\n",
       "                                              topics lewis_split  \\\n",
       "0                                          ['cocoa']     \"TRAIN\"   \n",
       "1  ['grain' 'wheat' 'corn' 'barley' 'oat' 'sorghum']     \"TRAIN\"   \n",
       "2  ['veg-oil' 'linseed' 'lin-oil' 'soy-oil' 'sun-...     \"TRAIN\"   \n",
       "3                                                 []     \"TRAIN\"   \n",
       "4                                           ['earn']     \"TRAIN\"   \n",
       "\n",
       "       cgis_split  old_id new_id                           places people orgs  \\\n",
       "0  \"TRAINING-SET\"  \"5544\"    \"1\"  ['el-salvador' 'usa' 'uruguay']     []   []   \n",
       "1  \"TRAINING-SET\"  \"5548\"    \"5\"                          ['usa']     []   []   \n",
       "2  \"TRAINING-SET\"  \"5549\"    \"6\"                    ['argentina']     []   []   \n",
       "3  \"TRAINING-SET\"  \"5551\"    \"8\"                          ['usa']     []   []   \n",
       "4  \"TRAINING-SET\"  \"5552\"    \"9\"                          ['usa']     []   []   \n",
       "\n",
       "  exchanges                     date  \\\n",
       "0        []  26-FEB-1987 15:01:01.79   \n",
       "1        []  26-FEB-1987 15:10:44.60   \n",
       "2        []  26-FEB-1987 15:14:36.41   \n",
       "3        []  26-FEB-1987 15:15:40.12   \n",
       "4        []  26-FEB-1987 15:17:11.20   \n",
       "\n",
       "                                              title  \n",
       "0                                BAHIA COCOA REVIEW  \n",
       "1  NATIONAL AVERAGE PRICES FOR FARMER-OWNED RESERVE  \n",
       "2     ARGENTINE 1986/87 GRAIN/OILSEED REGISTRATIONS  \n",
       "3              USX &lt;X> DEBT DOWGRADED BY MOODY'S  \n",
       "4    CHAMPION PRODUCTS &lt;CH> APPROVES STOCK SPLIT  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_encoder=False\n",
    "load_encoder=True\n",
    "save_encoder=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define treatment of columns und topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define topics\n",
    "topic_column = 'topics'\n",
    "food = ['coconut', 'cotton-oil', 'sorghum', 'orange', 'rice', 'soybean', 'sun-meal', \n",
    "    'oilseed', 'sugar', 'hog', 'coffee', 'groundnut', 'sunseed', 'sun-oil', 'rye', \n",
    "    'lin-oil', 'copra-cake', 'potato', 'barley', 'tea', 'meal-feed', 'coconut-oil', \n",
    "    'palmkernel', 'cottonseed', 'castor-oil', 'l-cattle', 'livestock', 'soy-oil', \n",
    "    'rape-oil', 'palm-oil', 'cocoa', 'cotton', 'wheat', 'corn', 'f-cattle', 'grain', \n",
    "    'soy-meal', 'oat', 'groundnut-oil', 'veg-oil','rapeseed']\n",
    "resource = ['platinum', 'lead', 'nickel', 'strategic-metal', 'copper', 'palladium', 'gold', \n",
    "    'zinc', 'tin', 'iron-steel', 'alum', 'silver', 'nat-gas', 'rubber', 'pet-chem', 'fuel', 'crude','lumber','propane','wool']\n",
    "finance = ['money-supply', 'dlr', 'nkr', 'lei', 'yen', 'dfl', 'sfr', 'cpi', 'instal-debt', \n",
    "    'money-fx', 'gnp', 'interest', 'income', 'dmk', 'rand', 'bop', 'reserves', 'nzdlr','acq']\n",
    "personal_finance = ['housing','jobs','earn']\n",
    "transport = ['jet', 'ship']\n",
    "topics=[[food,'food'],[resource,'resource'],[finance,'finance'],[personal_finance,'personal_finance'],[transport,'transport']]\n",
    "topics_to_remove = ['gas', 'heat', 'trade', 'retail', 'carcass', 'cpu', 'wpi', 'naphtha', 'ipi','stg','inventories']\n",
    "\n",
    "#columns with special treatment\n",
    "list_column='places'\n",
    "drop_columns=['text_type','people','orgs','exchanges','lewis_split','cgis_split','old_id','new_id']\n",
    "notnan_columns=['text','topics']\n",
    "date_columns=['date']\n",
    "text_columns=['text','title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define functions for Prepeocessing\n",
    "\n",
    "These may be turned into a library later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_row_notnan_columms(df,notnan_columns):\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    for column in notnan_columns:\n",
    "        df_copy[column].dropna(inplace=True)\n",
    "    \n",
    "    return df_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_listcolumns(df, column):\n",
    "    \"\"\"\n",
    "    Wandelt eine Spalte mit Listen als Strings formatiert in echte Listen um und gibt ein DataFrame und die eindeutigen Werte zurück.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Der DataFrame, der die Spalte enthält.\n",
    "    column (str): Der Name der Spalte, die konvertiert werden soll.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Das DataFrame mit der umgewandelten Spalte.\n",
    "    list: Eine Liste der eindeutigen Werte in der umgewandelten Spalte.\n",
    "\n",
    "    Example:\n",
    "    >>> df, unique_values = format_listcolumns(df_train, 'features')\n",
    "    \"\"\"\n",
    "    # Kopie der Spalte erstellen, um die Originaldaten nicht zu ändern\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # Umwandlung der Spalte von einem String in eine Liste\n",
    "    df_copy[column].replace({'\\\\n': ''}, regex=True, inplace=True)\n",
    "    df_copy = mytools.df_string_to_list(df_copy, column, entry_delimiter=\"'\", separator=' ')\n",
    "\n",
    "    # Eindeutige Werte in der umgewandelten Spalte finden\n",
    "    unique_values = mytools.df_unique_list_values(df_copy, column)\n",
    "\n",
    "    return df_copy, unique_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funtion to reorganize a column of subtopics into  broader topics and removing some of them \n",
    "def categorize_topics(df,column,topics,remove):\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    for topic in topics:\n",
    "        for subtopic in topic[0]:\n",
    "            df_copy[column] = df_copy[column].replace({'\\'' + subtopic + '\\'': '\\'' + topic[1] + '\\''}, regex=True)\n",
    "    \n",
    "    for subtopic in remove:\n",
    "        df_copy[column] = df_copy[column].replace({'\\'' + subtopic + '\\'': ''}, regex=True)\n",
    "    \n",
    "    df_copy[column] = df_copy[column].replace({' ': ''}, regex=True)\n",
    "    df_copy[column] = series_to_list(df_copy, column)\n",
    "    df_copy = df_copy[df_copy[column].str.len() == 1]\n",
    "    df_copy[column] = df_copy[column].apply(lambda x: x[0])\n",
    "    \n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_datecolumns(df,date_columns):\n",
    "    # Kopie des DataFrame erstellen, um die Originaldaten nicht zu ändern\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    for column in date_columns:\n",
    "        # Die Zeichenkette in ein Datum konvertieren\n",
    "        df_copy[column] = pd.to_datetime(df_copy[column].str.strip().str.split(' ').str.get(0))\n",
    "        df_copy[column+'_month'] = df_copy[column].dt.month\n",
    "        df_copy[column+'_month'] = (df_copy[column+'_month'] - df_copy[column+'_month'].mean()) / df_copy[column+'_month'].std()\n",
    "\n",
    "        # Woche extrahieren (altes Verhalten, ab Pandas 1.1.0 ist isocalendar().week empfohlen)\n",
    "        df_copy[column+'_day_month'] = df_copy[column].dt.day\n",
    "        df_copy[column+'_day_month'] = (df_copy[column+'_day_month'] - df_copy[column+'_day_month'].mean()) / df_copy[column+'_day_month'].std()\n",
    "\n",
    "        # Tag extrahieren\n",
    "        df_copy[column+'_day_year'] = df_copy[column].dt.day_of_year\n",
    "        df_copy[column+'_day_year'] = (df_copy[column+'_day_year'] - df_copy[column+'_day_year'].mean()) / df_copy[column+'_day_year'].std()\n",
    "\n",
    "        # Wochentag extrahieren (Montag=0, Sonntag=6)\n",
    "        df_copy[column+'_weekday'] = df_copy[column].dt.day_name('en')\n",
    "\n",
    "        df_copy[column+'_quarter_year'] = df_copy[column].dt.quarter\n",
    "        df_copy = pd.get_dummies(df_copy, columns=[column+'_weekday'])\n",
    "        weekdays = ['weekday_Monday', 'weekday_Tuesday', 'weekday_Wednesday', 'weekday_Thursday', 'weekday_Friday', 'weekday_Saturday', 'weekday_Sunday']\n",
    "        for weekday in weekdays:\n",
    "            if not column+'_'+weekday in df_copy.columns:\n",
    "                df_copy[column+'_'+weekday] = 0\n",
    "            else:\n",
    "                df_copy[column+'_'+weekday] = df_copy[column+'_'+weekday].astype(int)\n",
    "\n",
    "        df_copy = df_copy.drop(columns=column)\n",
    "\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_textcolumns(df,text_columns):\n",
    "    # Kopie des DataFrame erstellen, um die Originaldaten nicht zu ändern\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    for column in text_columns:\n",
    "        df_copy[column].replace({'&lt;': '<'}, regex=True, inplace=True)\n",
    "        df_copy[column].replace({'\\\\n': ' '}, regex=True, inplace=True)\n",
    "        df_copy[column] = df_copy[column].str.replace('\\s+', ' ', regex=True)\n",
    "        df_copy[column] = df_copy[column].str.lower()\n",
    "        df_copy[column] = df_copy[column].fillna(value='')\n",
    "\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_special_columns(df,list_column,list_possible_values,drop_columns,date_columns,notnan_columns,text_columns):\n",
    "    # Kopie des DataFrame erstellen, um die Originaldaten nicht zu ändern\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # Spalten aus dem DataFrame entfernen\n",
    "    df_copy = df_copy.drop(columns=drop_columns)\n",
    "\n",
    "    # Spalte mit Listen explodieren und mögliche Werte festlegen\n",
    "    df_copy = mytools.df_explode_listcolumn(df_copy, list_column, list_possible_values)\n",
    "\n",
    "    # Datumsangaben formatieren\n",
    "    df_copy = format_datecolumns(df_copy, date_columns)\n",
    "\n",
    "    # Zeilen entfernen, die NaN-Werte in bestimmten Spalten enthalten\n",
    "    df_copy = drop_row_notnan_columms(df_copy, notnan_columns)\n",
    "\n",
    "    # Textspalten formatieren\n",
    "    df_copy = format_textcolumns(df_copy, text_columns)\n",
    "\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#has to expanded to make it readeble by model\n",
    "def preprocessing(df,topic_column,topics,topics_to_remove,list_column,list_possible_values,drop_columns,date_columns,notnan_columns,text_columns,encoder,fit_encoder=True):\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    df_copy = categorize_topics(df_copy, topic_column, topics, topics_to_remove)\n",
    "    df_copy = handle_special_columns(df_copy, list_column, list_possible_values, drop_columns, date_columns, notnan_columns, text_columns)\n",
    "    \n",
    "    additional_features = torch.tensor(df_copy.drop(columns=(text_columns + [topic_column])).values)\n",
    "    additional_features = additional_features.float()\n",
    "    \n",
    "    if fit_encoder:\n",
    "        labels = torch.tensor(encoder.extend_transform(df_copy[topic_column]))\n",
    "    else:\n",
    "        labels = torch.tensor(encoder.transform(df_copy[topic_column]))\n",
    "    labels = labels.long()\n",
    "    \n",
    "    return df_copy[text_columns], additional_features, labels, encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actual Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\theinrich\\AppData\\Local\\Temp\\ipykernel_19256\\2357064120.py:20: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_copy[column].replace({'\\\\n': ''}, regex=True, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df,unique_values_test = format_listcolumns(df_test,list_column)\n",
    "df,unique_values_train = format_listcolumns(df_train,list_column)\n",
    "unique_countries=unique_values_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = mytools.LabelEncoder()\n",
    "label_encoder.extend_transform\n",
    "\n",
    "if load_encoder:\n",
    "    label_encoder.load('encoder.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\theinrich\\AppData\\Local\\Temp\\ipykernel_19256\\4235866303.py:7: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_copy[column] = pd.to_datetime(df_copy[column].str.strip().str.split(' ').str.get(0))\n",
      "C:\\Users\\theinrich\\AppData\\Local\\Temp\\ipykernel_19256\\1029708371.py:6: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_copy[column].replace({'&lt;': '<'}, regex=True, inplace=True)\n",
      "C:\\Users\\theinrich\\AppData\\Local\\Temp\\ipykernel_19256\\4235866303.py:7: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_copy[column] = pd.to_datetime(df_copy[column].str.strip().str.split(' ').str.get(0))\n",
      "C:\\Users\\theinrich\\AppData\\Local\\Temp\\ipykernel_19256\\1029708371.py:6: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_copy[column].replace({'&lt;': '<'}, regex=True, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_df_text,train_additional_features,train_labels,label_encoder = preprocessing(df_train,topic_column,topics,topics_to_remove,list_column,unique_countries,drop_columns,date_columns,notnan_columns,text_columns,label_encoder,fit_encoder=fit_encoder)\n",
    "\n",
    "test_df_text,test_additional_features,test_labels,label_encoder = preprocessing(df_test,topic_column,topics,topics_to_remove,list_column,unique_countries,drop_columns,date_columns,notnan_columns,text_columns,label_encoder,fit_encoder=fit_encoder)\n",
    "\n",
    "if save_encoder:\n",
    "    label_encoder.save('encoder.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Erstellung des Modell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definieren der Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "model_name='bert-base-multilingual-uncased'\n",
    "num_additional_features=139\n",
    "num_classes=5\n",
    "freeze_bert=True\n",
    "num_epochs=5\n",
    "batch_size=32\n",
    "model_path_base='models/Bert_freeze'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initilisieren vom Tokennizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definieren der benötigten Funktionen und Objekte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizen und Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_texts(text,length=128):\n",
    "    tokenized_text = tokenizer(text, return_tensors='pt', padding='max_length', truncation=True, max_length=length)\n",
    "    return tokenized_text\n",
    "\n",
    "def tokenize_inputs(df_text):\n",
    "    tokenized_inputs1 = []\n",
    "    tokenized_inputs2 = []\n",
    "    for idx, row in df_text.iterrows():\n",
    "        inputs1 = tokenize_texts(row['text'],256)\n",
    "        inputs2 = tokenize_texts(row['title'],16)\n",
    "        tokenized_inputs1.append(inputs1)\n",
    "        tokenized_inputs2.append(inputs2)\n",
    "    return tokenized_inputs1,tokenized_inputs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Text_Text_Feature_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokenized_inputs1, tokenized_inputs2, additional_features, labels):\n",
    "        self.tokenized_inputs1 = tokenized_inputs1\n",
    "        self.tokenized_inputs2 = tokenized_inputs2\n",
    "        self.additional_features = additional_features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids1 = self.tokenized_inputs1[idx]['input_ids'].squeeze()\n",
    "        attention_mask1 = self.tokenized_inputs1[idx]['attention_mask'].squeeze()\n",
    "        input_ids2 = self.tokenized_inputs2[idx]['input_ids'].squeeze()\n",
    "        attention_mask2 = self.tokenized_inputs2[idx]['attention_mask'].squeeze()\n",
    "        additional_features = self.additional_features[idx]\n",
    "        label = self.labels[idx]\n",
    "        return input_ids1, attention_mask1, input_ids2, attention_mask2, additional_features, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilingualBERTClassifier(nn.Module):\n",
    "    def __init__(self, bert_model_name='bert-base-multilingual-uncased', num_additional_features=119, num_classes=5, freeze_bert=True):\n",
    "        super(MultilingualBERTClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        \n",
    "        # Einfrieren der BERT-Gewichte, falls angegeben\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        self.additional_features_layer = nn.Linear(num_additional_features, 128)\n",
    "        self.classifier = nn.Linear(768 * 2 + 128, num_classes)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, input_ids1, attention_mask1, input_ids2, attention_mask2, additional_features):\n",
    "        outputs1 = self.bert(input_ids1, attention_mask=attention_mask1)\n",
    "        pooled_output1 = outputs1[1]  # [CLS] token representation\n",
    "        \n",
    "        outputs2 = self.bert(input_ids2, attention_mask=attention_mask2)\n",
    "        pooled_output2 = outputs2[1]  # [CLS] token representation\n",
    "        \n",
    "        additional_features_output = self.additional_features_layer(additional_features)\n",
    "        additional_features_output = torch.relu(additional_features_output)\n",
    "        \n",
    "        combined_output = torch.cat((pooled_output1, pooled_output2, additional_features_output), dim=1)\n",
    "        combined_output = self.dropout(combined_output)\n",
    "        \n",
    "        logits = self.classifier(combined_output)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Erstellen des Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize Text\n",
    "train_tokenized_inputs1, train_tokenized_inputs2= tokenize_inputs(train_df_text)\n",
    "test_tokenized_inputs1, test_tokenized_inputs2= tokenize_inputs(test_df_text)\n",
    "#Create Datasets\n",
    "train_dataset = Text_Text_Feature_Dataset(train_tokenized_inputs1, train_tokenized_inputs2,train_additional_features,train_labels)\n",
    "test_dataset = Text_Text_Feature_Dataset(test_tokenized_inputs1, test_tokenized_inputs2,test_additional_features,test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialisieren von Modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No versions available.\n",
      "Model loaded and on device\n"
     ]
    }
   ],
   "source": [
    "#Create modell or load previous one\n",
    "model = MultilingualBERTClassifier(num_additional_features=num_additional_features, num_classes=num_classes)\n",
    "#load_model(model,model_path_base)\n",
    "mytools.modelversions_load_model(model,model_path_base)\n",
    "# move modell to divice if possible\n",
    "model.to(device)\n",
    "print('Model loaded and on device')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create Dataloader\n",
    "dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# Loss-Funktion and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 199/199 [00:59<00:00,  3.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 (BERT eingefroren), Loss: 1.1001562727755638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 199/199 [00:59<00:00,  3.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 (BERT eingefroren), Loss: 1.0979603894391852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 199/199 [01:01<00:00,  3.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 (BERT eingefroren), Loss: 1.0939032968564248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 199/199 [01:00<00:00,  3.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 (BERT eingefroren), Loss: 1.092449508420187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 199/199 [01:00<00:00,  3.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 (BERT eingefroren), Loss: 1.0834619058436485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for batch in tqdm(dataloader, desc=f'Epoch {epoch + 1}'):\n",
    "        input_ids1, attention_mask1, input_ids2, attention_mask2, additional_features, labels = batch\n",
    "        input_ids1, attention_mask1 = input_ids1.to(device), attention_mask1.to(device)\n",
    "        input_ids2, attention_mask2 = input_ids2.to(device), attention_mask2.to(device)\n",
    "        additional_features, labels = additional_features.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Vorwärtsdurchlauf\n",
    "        logits = model(input_ids1, attention_mask1, input_ids2, attention_mask2, additional_features)\n",
    "\n",
    "        # Verlust berechnen\n",
    "        loss = criterion(logits, labels)\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        # Rückwärtsdurchlauf und Optimierung\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1} (BERT eingefroren), Loss: {epoch_loss / len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [21:20<00:00, 16.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.08619313647246608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#test modell\n",
    "model.eval()\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "test_predictions = []\n",
    "test_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_dataloader):\n",
    "        input_ids1, attention_mask1, input_ids2, attention_mask2, additional_features, labels = batch\n",
    "        input_ids1, attention_mask1 = input_ids1.to(device), attention_mask1.to(device)\n",
    "        input_ids2, attention_mask2 = input_ids2.to(device), attention_mask2.to(device)\n",
    "        additional_features, labels = additional_features.to(device), labels.to(device)\n",
    "\n",
    "        logits = model(input_ids1, attention_mask1, input_ids2, attention_mask2, additional_features)\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        \n",
    "        test_predictions.extend(predictions.cpu().numpy())\n",
    "        test_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "test_predictions = label_encoder.inverse_transform(test_predictions)\n",
    "test_labels = label_encoder.inverse_transform(test_labels)\n",
    "\n",
    "accuracy = sum(test_predictions == test_labels) / len(test_labels)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        TP abs True Positive       TP %  FP abs  \\\n",
      "Epoch Class                                                       \n",
      "1     food                  29   22.83% (29)  22.834646     556   \n",
      "      resource             146  55.09% (146)  55.094340    1683   \n",
      "      finance               40    4.09% (40)   4.085802      37   \n",
      "      personal_finance       0      0.0% (0)   0.000000       0   \n",
      "      transport              1      2.7% (1)   2.702703      14   \n",
      "2     food                  29   22.83% (29)  22.834646     556   \n",
      "      resource             146  55.09% (146)  55.094340    1683   \n",
      "      finance               40    4.09% (40)   4.085802      37   \n",
      "      personal_finance       0      0.0% (0)   0.000000       0   \n",
      "      transport              1      2.7% (1)   2.702703      14   \n",
      "\n",
      "                        False Positive        FP %  FN abs False Negative  \\\n",
      "Epoch Class                                                                 \n",
      "1     food                437.8% (556)  437.795276      98    77.17% (98)   \n",
      "      resource          635.09% (1683)  635.094340     119   44.91% (119)   \n",
      "      finance               3.78% (37)    3.779367     939   95.91% (939)   \n",
      "      personal_finance        0.0% (0)    0.000000    1098  100.0% (1098)   \n",
      "      transport            37.84% (14)   37.837838      36     97.3% (36)   \n",
      "2     food                437.8% (556)  437.795276      98    77.17% (98)   \n",
      "      resource          635.09% (1683)  635.094340     119   44.91% (119)   \n",
      "      finance               3.78% (37)    3.779367     939   95.91% (939)   \n",
      "      personal_finance        0.0% (0)    0.000000    1098  100.0% (1098)   \n",
      "      transport            37.84% (14)   37.837838      36     97.3% (36)   \n",
      "\n",
      "                              FN %  \n",
      "Epoch Class                         \n",
      "1     food               77.165354  \n",
      "      resource           44.905660  \n",
      "      finance            95.914198  \n",
      "      personal_finance  100.000000  \n",
      "      transport          97.297297  \n",
      "2     food               77.165354  \n",
      "      resource           44.905660  \n",
      "      finance            95.914198  \n",
      "      personal_finance  100.000000  \n",
      "      transport          97.297297  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "classes=[topic[1] for topic in topics]\n",
    "def calculate_and_append_metrics(epoch, classes, labels, predictions,df=None):\n",
    "    # Confusion Matrix erstellen\n",
    "    cm = confusion_matrix(labels, predictions, labels=classes)\n",
    "\n",
    "    # Spalten für das DataFrame definieren\n",
    "    columns = ['Epoch', 'Class', 'TP abs', 'True Positive', 'TP %', 'FP abs', 'False Positive', 'FP %', 'FN abs','False Negative', 'FN %']\n",
    "    data = []\n",
    "\n",
    "    # Iteration über die Klassen\n",
    "    for idx, class_name in enumerate(classes):\n",
    "        total = sum(1 for label in labels if label == class_name)\n",
    "        true_positive = cm[idx, idx]\n",
    "        false_positive = cm[:, idx].sum() - true_positive\n",
    "        false_negative = cm[idx,:].sum() - true_positive\n",
    "\n",
    "        tp_percent = (true_positive / total) * 100 if total > 0 else 0\n",
    "        fp_percent = (false_positive / total) * 100 if total > 0 else 0\n",
    "        fn_percent = (false_negative / total) * 100 if total > 0 else 0\n",
    "\n",
    "        tp_text = f\"{round(tp_percent,2)}% ({true_positive})\"\n",
    "        fp_text = f\"{round(fp_percent,2)}% ({false_positive})\"\n",
    "        fn_text = f\"{round(fn_percent,2)}% ({false_negative})\"\n",
    "\n",
    "\n",
    "        data.append([epoch, class_name, true_positive, tp_text, tp_percent, false_positive, fp_text, fp_percent, false_negative, fn_text, fn_percent])\n",
    "\n",
    "    # DataFrame für die aktuelle Epoche erstellen\n",
    "    epoch_df = pd.DataFrame(data, columns=columns)\n",
    "    epoch_df.set_index(['Epoch', 'Class'],inplace=True)\n",
    "    if df is None:\n",
    "        return epoch_df\n",
    "    else:\n",
    "        return pd.concat([df, epoch_df])\n",
    "    \n",
    "df=calculate_and_append_metrics(1, classes, test_labels, test_predictions)\n",
    "#est labels und prediction vertauscht um unterschiedliche Daen zu haben\n",
    "df=calculate_and_append_metrics(2, classes, test_labels, test_predictions, df)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>finance</th>\n",
       "      <th>food</th>\n",
       "      <th>personal_finance</th>\n",
       "      <th>resource</th>\n",
       "      <th>transport</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">1</th>\n",
       "      <th>True Positive</th>\n",
       "      <td>4.09% (40)</td>\n",
       "      <td>22.83% (29)</td>\n",
       "      <td>0.0% (0)</td>\n",
       "      <td>55.09% (146)</td>\n",
       "      <td>2.7% (1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False Positive</th>\n",
       "      <td>3.78% (37)</td>\n",
       "      <td>437.8% (556)</td>\n",
       "      <td>0.0% (0)</td>\n",
       "      <td>635.09% (1683)</td>\n",
       "      <td>37.84% (14)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False Negative</th>\n",
       "      <td>95.91% (939)</td>\n",
       "      <td>77.17% (98)</td>\n",
       "      <td>100.0% (1098)</td>\n",
       "      <td>44.91% (119)</td>\n",
       "      <td>97.3% (36)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">2</th>\n",
       "      <th>True Positive</th>\n",
       "      <td>4.09% (40)</td>\n",
       "      <td>22.83% (29)</td>\n",
       "      <td>0.0% (0)</td>\n",
       "      <td>55.09% (146)</td>\n",
       "      <td>2.7% (1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False Positive</th>\n",
       "      <td>3.78% (37)</td>\n",
       "      <td>437.8% (556)</td>\n",
       "      <td>0.0% (0)</td>\n",
       "      <td>635.09% (1683)</td>\n",
       "      <td>37.84% (14)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False Negative</th>\n",
       "      <td>95.91% (939)</td>\n",
       "      <td>77.17% (98)</td>\n",
       "      <td>100.0% (1098)</td>\n",
       "      <td>44.91% (119)</td>\n",
       "      <td>97.3% (36)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Class                      finance          food personal_finance  \\\n",
       "Epoch                                                               \n",
       "1     True Positive     4.09% (40)   22.83% (29)         0.0% (0)   \n",
       "      False Positive    3.78% (37)  437.8% (556)         0.0% (0)   \n",
       "      False Negative  95.91% (939)   77.17% (98)    100.0% (1098)   \n",
       "2     True Positive     4.09% (40)   22.83% (29)         0.0% (0)   \n",
       "      False Positive    3.78% (37)  437.8% (556)         0.0% (0)   \n",
       "      False Negative  95.91% (939)   77.17% (98)    100.0% (1098)   \n",
       "\n",
       "Class                       resource    transport  \n",
       "Epoch                                              \n",
       "1     True Positive     55.09% (146)     2.7% (1)  \n",
       "      False Positive  635.09% (1683)  37.84% (14)  \n",
       "      False Negative    44.91% (119)   97.3% (36)  \n",
       "2     True Positive     55.09% (146)     2.7% (1)  \n",
       "      False Positive  635.09% (1683)  37.84% (14)  \n",
       "      False Negative    44.91% (119)   97.3% (36)  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_df = df.loc[df.index.get_level_values('Epoch') >= (df.index.get_level_values('Epoch').max() - 2)]\n",
    "columns = pd.MultiIndex.from_product([metrics_df.columns.get_level_values(0).unique(), ['True Positive', 'False Positive', 'False Negative']])\n",
    "metrics_table = pd.DataFrame(index=metrics_df.index.levels[0], columns=columns)\n",
    "metrics_df = metrics_df[['True Positive','False Positive','False Negative']]\n",
    "metrics_df.stack().unstack(level=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "(1, 'True Positive')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'True Positive'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mindex.pyx:768\u001b[0m, in \u001b[0;36mpandas._libs.index.BaseMultiIndexCodesEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'True Positive'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\multi.py:3220\u001b[0m, in \u001b[0;36mMultiIndex._get_loc_level\u001b[1;34m(self, key, level)\u001b[0m\n\u001b[0;32m   3219\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m   3221\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:771\u001b[0m, in \u001b[0;36mpandas._libs.index.BaseMultiIndexCodesEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: (1, 'True Positive')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[91], line 37\u001b[0m\n\u001b[0;32m     34\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Plotte die Tabelle\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m \u001b[43mplot_metrics_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[91], line 13\u001b[0m, in \u001b[0;36mplot_metrics_table\u001b[1;34m(metrics_df)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m metrics_df\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mlevels[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m class_name \u001b[38;5;129;01min\u001b[39;00m metrics_df\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_level_values(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39munique():\n\u001b[1;32m---> 13\u001b[0m         tp \u001b[38;5;241m=\u001b[39m \u001b[43mmetrics_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTP \u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     14\u001b[0m         tp_abs \u001b[38;5;241m=\u001b[39m metrics_df\u001b[38;5;241m.\u001b[39mloc[(epoch, class_name), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrue Positive\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     15\u001b[0m         fp \u001b[38;5;241m=\u001b[39m metrics_df\u001b[38;5;241m.\u001b[39mloc[(epoch, class_name), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFP \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexing.py:1184\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_scalar_access(key):\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_value(\u001b[38;5;241m*\u001b[39mkey, takeable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_takeable)\n\u001b[1;32m-> 1184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1186\u001b[0m     \u001b[38;5;66;03m# we by definition only have the 0th axis\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m     axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexing.py:1368\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_tuple\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m   1366\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m suppress(IndexingError):\n\u001b[0;32m   1367\u001b[0m     tup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_ellipsis(tup)\n\u001b[1;32m-> 1368\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_lowerdim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1370\u001b[0m \u001b[38;5;66;03m# no multi-index, so validate all of the indexers\u001b[39;00m\n\u001b[0;32m   1371\u001b[0m tup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_tuple_indexer(tup)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexing.py:1041\u001b[0m, in \u001b[0;36m_LocationIndexer._getitem_lowerdim\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;66;03m# we may have a nested tuples indexer here\u001b[39;00m\n\u001b[0;32m   1040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_nested_tuple_indexer(tup):\n\u001b[1;32m-> 1041\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_nested_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1043\u001b[0m \u001b[38;5;66;03m# we maybe be using a tuple to represent multiple dimensions here\u001b[39;00m\n\u001b[0;32m   1044\u001b[0m ax0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexing.py:1153\u001b[0m, in \u001b[0;36m_LocationIndexer._getitem_nested_tuple\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     axis \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1151\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m-> 1153\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1154\u001b[0m axis \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1156\u001b[0m \u001b[38;5;66;03m# if we have a scalar, we are done\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexing.py:1431\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1429\u001b[0m \u001b[38;5;66;03m# fall thru to straight lookup\u001b[39;00m\n\u001b[0;32m   1430\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_key(key, axis)\n\u001b[1;32m-> 1431\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_label\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexing.py:1381\u001b[0m, in \u001b[0;36m_LocIndexer._get_label\u001b[1;34m(self, label, axis)\u001b[0m\n\u001b[0;32m   1379\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_label\u001b[39m(\u001b[38;5;28mself\u001b[39m, label, axis: AxisInt):\n\u001b[0;32m   1380\u001b[0m     \u001b[38;5;66;03m# GH#5567 this will fail if the label is not present in the axis.\u001b[39;00m\n\u001b[1;32m-> 1381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py:4293\u001b[0m, in \u001b[0;36mNDFrame.xs\u001b[1;34m(self, key, axis, level, drop_level)\u001b[0m\n\u001b[0;32m   4290\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\n\u001b[0;32m   4292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(index, MultiIndex):\n\u001b[1;32m-> 4293\u001b[0m     loc, new_index \u001b[38;5;241m=\u001b[39m \u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_loc_level\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4294\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m drop_level:\n\u001b[0;32m   4295\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mis_integer(loc):\n\u001b[0;32m   4296\u001b[0m             \u001b[38;5;66;03m# Slice index must be an integer or None\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\multi.py:3222\u001b[0m, in \u001b[0;36mMultiIndex._get_loc_level\u001b[1;34m(self, key, level)\u001b[0m\n\u001b[0;32m   3220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(key), \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m   3221\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3222\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3223\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3224\u001b[0m     \u001b[38;5;66;03m# e.g. partial string indexing\u001b[39;00m\n\u001b[0;32m   3225\u001b[0m     \u001b[38;5;66;03m#  test_partial_string_timestamp_multiindex\u001b[39;00m\n\u001b[0;32m   3226\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: (1, 'True Positive')"
     ]
    }
   ],
   "source": [
    "# Funktion zur Erstellung der geplotteten Tabelle\n",
    "def plot_metrics_table(metrics_df):\n",
    "    # Nur die letzten 3 Epochen anzeigen\n",
    "    metrics_df = metrics_df.loc[metrics_df.index.get_level_values('Epoch') >= (metrics_df.index.get_level_values('Epoch').max() - 2)]\n",
    "    \n",
    "    # MultiIndex für Spalten erstellen\n",
    "    columns = pd.MultiIndex.from_product([metrics_df.columns.get_level_values(0).unique(), ['True Positive', 'False Positive', 'False Negative']])\n",
    "    metrics_table = pd.DataFrame(index=metrics_df.index.levels[0], columns=columns)\n",
    "\n",
    "    # Werte formatieren und in die neue Tabelle einfügen\n",
    "    for epoch in metrics_df.index.levels[0]:\n",
    "        for class_name in metrics_df.columns.get_level_values(0).unique():\n",
    "            tp = metrics_df.loc[(epoch, class_name), 'TP %']\n",
    "            tp_abs = metrics_df.loc[(epoch, class_name), 'True Positive']\n",
    "            fp = metrics_df.loc[(epoch, class_name), 'FP %']\n",
    "            fp_abs = metrics_df.loc[(epoch, class_name), 'False Positive']\n",
    "            fn = metrics_df.loc[(epoch, class_name), 'FN %']\n",
    "            fn_abs = metrics_df.loc[(epoch, class_name), 'False Negative']\n",
    "\n",
    "            metrics_table.loc[epoch, (class_name, 'True Positive')] = f\"{tp:.1f}% ({tp_abs})\"\n",
    "            metrics_table.loc[epoch, (class_name, 'False Positive')] = f\"{fp:.1f}% ({fp_abs})\"\n",
    "            metrics_table.loc[epoch, (class_name, 'False Negative')] = f\"{fn:.1f}% ({fn_abs})\"\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.axis('tight')\n",
    "    ax.axis('off')\n",
    "\n",
    "    table = ax.table(cellText=metrics_table.values,\n",
    "                     colLabels=metrics_table.columns,\n",
    "                     rowLabels=metrics_table.index,\n",
    "                     cellLoc='center',\n",
    "                     loc='center')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Plotte die Tabelle\n",
    "plot_metrics_table(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Beispiel DataFrame (hier kannst du dein eigenes DataFrame verwenden)\n",
    "data = {\n",
    "    'Epoch': [1, 1, 2, 2, 3, 3, 4, 4, 5, 5],\n",
    "    'Class': ['A', 'B', 'A', 'B', 'A', 'B', 'A', 'B', 'A', 'B'],\n",
    "    'True Positive': [100, 120, 110, 130, 115, 135, 105, 125, 120, 140],\n",
    "    'TP %': [80.0, 85.0, 82.5, 87.5, 78.0, 89.5, 75.0, 80.5, 82.0, 90.0],\n",
    "    'False Positive': [20, 18, 22, 20, 24, 23, 21, 19, 18, 16],\n",
    "    'FP %': [10.0, 9.0, 11.0, 10.0, 12.0, 11.5, 10.5, 9.5, 9.0, 8.0],\n",
    "    'False Negative': [30, 28, 35, 32, 32, 30, 28, 25, 30, 27],\n",
    "    'FN %': [15.0, 14.0, 17.5, 16.0, 16.0, 15.0, 14.0, 12.5, 15.0, 13.5]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Die letzten drei Epochen auswählen\n",
    "last_three_epochs = df[df['Epoch'].isin(df['Epoch'].unique()[-3:])]\n",
    "\n",
    "# Spalten auswählen, die für den Gradienten relevant sind (TP %, FP %, FN %)\n",
    "gradient_columns = ['TP %', 'FP %', 'FN %']\n",
    "\n",
    "# Konvertiere die relevanten Spalten in ein Array\n",
    "data_array = last_three_epochs.pivot(index='Class', columns='Epoch', values=gradient_columns).values\n",
    "\n",
    "# Berechne die Änderungen zum vorherigen Eintrag\n",
    "diffs = np.diff(data_array, axis=1)\n",
    "\n",
    "# Setze die Farbpalette für den Gradienten\n",
    "cmap = sns.diverging_palette(220, 20, as_cmap=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[78. , 75. , 82. , 12. , 10.5,  9. , 16. , 14. , 15. ],\n",
       "       [89.5, 80.5, 90. , 11.5,  9.5,  8. , 15. , 12.5, 13.5]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 2-dimensional, but 3 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Plot erstellen\u001b[39;00m\n\u001b[0;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m----> 3\u001b[0m sns\u001b[38;5;241m.\u001b[39mheatmap(\u001b[43mdiffs\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m, annot\u001b[38;5;241m=\u001b[39mdata_array[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :], fmt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;124m\"\u001b[39m, cmap\u001b[38;5;241m=\u001b[39mcmap, cbar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      4\u001b[0m             xticklabels\u001b[38;5;241m=\u001b[39mgradient_columns, yticklabels\u001b[38;5;241m=\u001b[39mlast_three_epochs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClass\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique(),\n\u001b[0;32m      5\u001b[0m             linewidths\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m.5\u001b[39m, square\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, annot_kws\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m10\u001b[39m})\n\u001b[0;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGradient Plot der letzten drei Epochen\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMetrik\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for array: array is 2-dimensional, but 3 were indexed"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot erstellen\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(diffs[:, -1, :], annot=data_array[:, -1, :], fmt=\".1f\", cmap=cmap, cbar=True,\n",
    "            xticklabels=gradient_columns, yticklabels=last_three_epochs['Class'].unique(),\n",
    "            linewidths=.5, square=True, annot_kws={\"size\": 10})\n",
    "\n",
    "plt.title('Gradient Plot der letzten drei Epochen')\n",
    "plt.xlabel('Metrik')\n",
    "plt.ylabel('Klasse')\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>finance</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>food</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>personal_finance</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resource</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>transport</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      0\n",
       "1                      \n",
       "finance            True\n",
       "food              False\n",
       "personal_finance   True\n",
       "resource          False\n",
       "transport         False"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define categories\n",
    "categories = ['food', 'resource', 'finance', 'personal_finance', 'transport']\n",
    "accuracy_data = [test_predictions==test_labels,test_labels]\n",
    "#total datapoints\n",
    "total = len(test_labels)\n",
    "# DataFrame erstellen\n",
    "accuracy_df = pd.DataFrame(accuracy_data).T\n",
    "#group by and sum for each category and total\n",
    "accuracy_df = accuracy_df.groupby(1).sum()\n",
    "accuracy_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Tabelle plotten\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39msubplots()\n\u001b[0;32m      3\u001b[0m ax\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtight\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m ax\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Tabelle plotten\n",
    "fig, ax = plt.subplots()\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "table = ax.table(cellText=accuracy_df.values, colLabels=accuracy_df.columns, rowLabels=accuracy_df.index, loc='center', cellLoc='center')\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(14)\n",
    "table.scale(1.2, 1.2)\n",
    "\n",
    "plt.title('Accuracy für verschiedene Kategorien')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for finance: 0.6567926455566905\n",
      "Accuracy for food: 0.0\n",
      "Accuracy for personal_finance: 0.9052823315118397\n",
      "Accuracy for resource: 0.0\n",
      "Accuracy for transport: 0.0\n"
     ]
    }
   ],
   "source": [
    "#print accuracy by topic\n",
    "for topic in label_encoder.classes_:\n",
    "    topic_mask = test_labels == topic\n",
    "    topic_accuracy = sum(test_predictions[topic_mask] == test_labels[topic_mask]) / sum(topic_mask)\n",
    "    print(f'Accuracy for {topic}: {topic_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving Modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as models/Bert_freeze_v2.pt\n"
     ]
    }
   ],
   "source": [
    "mytools.modelversions_save_model(model, model_path_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "classes=[topic[1] for topic in topics]\n",
    "def calculate_and_append_metrics(epoch, classes, labels, predictions,df=None):\n",
    "    # Confusion Matrix erstellen\n",
    "    cm = confusion_matrix(labels, predictions, labels=classes)\n",
    "\n",
    "    # Spalten für das DataFrame definieren\n",
    "    columns = ['Epoch', 'Class', 'TP abs', 'True Positive', 'TP %', 'FP abs', 'False Positive', 'FP %', 'FN abs','False Negative', 'FN %']\n",
    "    data = []\n",
    "\n",
    "    # Iteration über die Klassen\n",
    "    for idx, class_name in enumerate(classes):\n",
    "        total = sum(1 for label in labels if label == class_name)\n",
    "        true_positive = cm[idx, idx]\n",
    "        false_positive = cm[:, idx].sum() - true_positive\n",
    "        false_negative = cm[idx,:].sum() - true_positive\n",
    "\n",
    "        tp_percent = (true_positive / total) * 100 if total > 0 else 0\n",
    "        fp_percent = (false_positive / total) * 100 if total > 0 else 0\n",
    "        fn_percent = (false_negative / total) * 100 if total > 0 else 0\n",
    "\n",
    "        tp_text = f\"{round(tp_percent,2)}% ({true_positive})\"\n",
    "        fp_text = f\"{round(fp_percent,2)}% ({false_positive})\"\n",
    "        fn_text = f\"{round(fn_percent,2)}% ({false_negative})\"\n",
    "\n",
    "\n",
    "        data.append([epoch, class_name, true_positive, tp_text, tp_percent, false_positive, fp_text, fp_percent, false_negative, fn_text, fn_percent])\n",
    "\n",
    "    # DataFrame für die aktuelle Epoche erstellen\n",
    "    epoch_df = pd.DataFrame(data, columns=columns)\n",
    "    epoch_df.set_index(['Epoch', 'Class'],inplace=True)\n",
    "    if df is None:\n",
    "        return epoch_df\n",
    "    else:\n",
    "        return pd.concat([df, epoch_df])\n",
    "    \n",
    "df=calculate_and_append_metrics(1, classes, test_labels, test_predictions)\n",
    "#est labels und prediction vertauscht um unterschiedliche Daen zu haben\n",
    "df=calculate_and_append_metrics(2, classes, test_labels, test_predictions, df)\n",
    "\n",
    "metrics_df = df.loc[df.index.get_level_values('Epoch') >= (df.index.get_level_values('Epoch').max() - 2)]\n",
    "columns = pd.MultiIndex.from_product([metrics_df.columns.get_level_values(0).unique(), ['True Positive', 'False Positive', 'False Negative']])\n",
    "metrics_table = pd.DataFrame(index=metrics_df.index.levels[0], columns=columns)\n",
    "metrics_df = metrics_df[['True Positive','False Positive','False Negative']]\n",
    "metrics_df.stack().unstack(level=1)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def calculate_class_weights(labels, method='sqrt'):\n",
    "    \"\"\"\n",
    "    Berechnet die Gewichte für die Klassen basierend auf der Häufigkeit der Labels und der angegebenen Methode.\n",
    "\n",
    "    Args:\n",
    "    labels (torch.Tensor): Tensor der Labels.\n",
    "    method (str): Methode zur Berechnung der Gewichte ('sqrt', 'log', 'inverse').\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: Tensor der Klassen-Gewichte.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Berechne die Häufigkeit jeder Klasse\n",
    "    unique_classes, class_counts = torch.unique(labels, return_counts=True)\n",
    "    \n",
    "    # Berechne die Gesamtzahl der Beispiele\n",
    "    total_samples = labels.size(0)\n",
    "    \n",
    "    if method == 'sqrt':\n",
    "        # Umgekehrt proportional zur Quadratwurzel der Klassenhäufigkeit\n",
    "        weights = torch.sqrt(total_samples / class_counts.float())\n",
    "    elif method == 'log':\n",
    "        # Logarithmische Gewichtung\n",
    "        weights = torch.log(1 + total_samples / class_counts.float())\n",
    "    elif method == 'inverse':\n",
    "        # Umgekehrt proportional zur Klassenhäufigkeit\n",
    "        weights = total_samples / class_counts.float()\n",
    "    else:\n",
    "        raise ValueError(\"Method must be 'sqrt', 'log', or 'inverse'\")\n",
    "    \n",
    "    # Normalisierung (optional)\n",
    "    weights = weights / torch.sum(weights) * len(unique_classes)\n",
    "    \n",
    "    # Erstelle eine Gewichtungstabelle für alle möglichen Klassen\n",
    "    class_weights = torch.zeros(len(unique_classes), dtype=torch.float)\n",
    "    class_weights[unique_classes] = weights\n",
    "    \n",
    "    return class_weights\n",
    "\n",
    "# Beispielaufruf\n",
    "labels = torch.tensor([0] * 2895 + [1] * 2379 + [2] * 556 + [3] * 393 + [4] * 111)\n",
    "class_weights_sqrt = calculate_class_weights(labels, method='sqrt')\n",
    "class_weights_log = calculate_class_weights(labels, method='log')\n",
    "class_weights_inverse = calculate_class_weights(labels, method='inverse')\n",
    "\n",
    "print(\"Class Weights (sqrt):\", class_weights_sqrt)\n",
    "print(\"Class Weights (log):\", class_weights_log)\n",
    "print(\"Class Weights (inverse):\", class_weights_inverse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
